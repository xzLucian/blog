# LangChain Retrieval
Retrievalç›´æ¥ç¿»è¯‘è¿‡æ¥å³â€œæ£€ç´¢â€ï¼Œæœ¬ç« Retrievalæ¨¡å—åŒ…æ‹¬ä¸æ£€ç´¢æ­¥éª¤ç›¸å…³çš„æ‰€æœ‰å†…å®¹ï¼Œä¾‹å¦‚æ•°æ®çš„è·å–ã€åˆ‡åˆ†ã€å‘é‡åŒ–ã€å‘é‡å­˜å‚¨ã€å‘é‡æ£€ç´¢ç­‰æ¨¡å—ã€‚å¸¸è¢«åº”ç”¨äºæ„å»ºä¸€ä¸ªâ€œ ä¼ä¸š/ç§äººçš„çŸ¥è¯†åº“ â€ï¼Œæå‡å¤§æ¨¡å‹çš„æ•´ä½“èƒ½åŠ›ã€‚

## 1. Retrievalæ¨¡å—çš„è®¾è®¡æ„ä¹‰
### 1.1 å¤§æ¨¡å‹çš„å¹»è§‰é—®é¢˜
æ‹¥æœ‰è®°å¿†åï¼Œç¡®å®æ‰©å±•äº†AIå·¥ç¨‹çš„åº”ç”¨åœºæ™¯ã€‚
ä½†æ˜¯åœ¨ä¸“æœ‰é¢†åŸŸï¼ŒLLMæ— æ³•å­¦ä¹ åˆ°æ‰€æœ‰çš„ä¸“ä¸šçŸ¥è¯†ç»†èŠ‚ï¼Œå› æ­¤åœ¨ é¢å‘ä¸“ä¸šé¢†åŸŸçŸ¥è¯† çš„æé—®æ—¶ï¼Œæ— æ³•ç»™å‡ºå¯é å‡†ç¡®çš„å›ç­”ï¼Œç”šè‡³ä¼šâ€œèƒ¡è¨€ä¹±è¯­â€ï¼Œè¿™ç§ç°è±¡ç§°ä¹‹ä¸º`LLMçš„â€œå¹»è§‰â€`ã€‚
å¤§æ¨¡å‹ç”Ÿæˆå†…å®¹çš„ä¸å¯æ§ï¼Œå°¤å…¶æ˜¯åœ¨é‡‘èå’ŒåŒ»ç–—é¢†åŸŸç­‰é¢†åŸŸï¼Œ**ä¸€æ¬¡é‡‘é¢è¯„ä¼°çš„é”™è¯¯ï¼Œä¸€æ¬¡åŒ»ç–—è¯Šæ–­çš„å¤±è¯¯ï¼Œå“ªæ€•åªå‡ºç°ä¸€æ¬¡éƒ½æ˜¯è‡´å‘½çš„**ã€‚ä½†å¯¹äºéä¸“ä¸šäººå£«æ¥è¯´å¯èƒ½éš¾ä»¥è¾¨è¯†ã€‚ç›®å‰è¿˜æ²¡æœ‰èƒ½å¤Ÿç™¾åˆ†ä¹‹ç™¾è§£å†³è¿™ç§æƒ…å†µçš„æ–¹æ¡ˆã€‚

**å½“å‰å¤§å®¶æ™®éè¾¾æˆå…±è¯†çš„ä¸€ä¸ªæ–¹æ¡ˆï¼š**
é¦–å…ˆï¼Œä¸ºå¤§æ¨¡å‹æä¾›ä¸€å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè®©å…¶è¾“å‡ºä¼šå˜å¾—æ›´ç¨³å®šã€‚
å…¶æ¬¡ï¼Œåˆ©ç”¨æœ¬ç« çš„RAGï¼Œå°†æ£€ç´¢å‡ºæ¥çš„ æ–‡æ¡£å’Œæç¤ºè¯ è¾“é€ç»™å¤§æ¨¡å‹ï¼Œç”Ÿæˆæ›´å¯é çš„ç­”æ¡ˆã€‚

### 1.2 RAGçš„è§£å†³æ–¹æ¡ˆ
å¯ä»¥è¯´ï¼Œå½“åº”ç”¨éœ€æ±‚é›†ä¸­åœ¨åˆ©ç”¨å¤§æ¨¡å‹å»**å›ç­”ç‰¹å®šç§æœ‰é¢†åŸŸçš„çŸ¥è¯†**ï¼Œä¸”çŸ¥è¯†åº“è¶³å¤Ÿå¤§ï¼Œé‚£ä¹ˆé™¤äº†`å¾®è°ƒå¤§æ¨¡å‹`å¤–ï¼Œ**RAG**å°±æ˜¯éå¸¸æœ‰æ•ˆçš„ä¸€ç§ç¼“è§£å¤§æ¨¡å‹æ¨ç†çš„â€œå¹»è§‰â€é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚
LangChainå¯¹è¿™ä¸€æµç¨‹æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚

> å¦‚æœè¯´LangChainç›¸å½“äºç»™LLMè¿™ä¸ªâ€œå¤§è„‘â€å®‰è£…äº†â€œå››è‚¢å’Œèº¯â¼²â€ï¼ŒRAGåˆ™æ˜¯ä¸ºLLMæä¾›äº†æ¥â¼Šâ€œ
â¼ˆç±»çŸ¥è¯†å›¾ä¹¦é¦†â€çš„èƒ½â¼’ã€‚

### 1.3 RAGçš„ä¼˜ç¼ºç‚¹
**RAGçš„ä¼˜ç‚¹**

1ï¼‰ç›¸æ¯”æç¤ºè¯å·¥ç¨‹ï¼ŒRAGæœ‰`æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡å’Œæ•°æ®æ ·æœ¬`ï¼Œå¯ä»¥ä¸éœ€è¦ç”¨æˆ·æä¾›è¿‡å¤šçš„èƒŒæ™¯æè¿°ï¼Œå°±èƒ½ç”Ÿæˆæ¯”è¾ƒç¬¦åˆç”¨æˆ·é¢„æœŸçš„ç­”æ¡ˆã€‚
2ï¼‰ç›¸æ¯”äºæ¨¡å‹å¾®è°ƒï¼ŒRAGå¯ä»¥æå‡é—®ç­”å†…å®¹çš„ `æ—¶æ•ˆæ€§` å’Œ `å¯é æ€§`ã€‚
3ï¼‰åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¿æŠ¤äº†ä¸šåŠ¡æ•°æ®çš„ `éšç§æ€§ã€‚

**RAGçš„ç¼ºç‚¹**

1ï¼‰ç”±äºæ¯æ¬¡é—®ç­”éƒ½æ¶‰åŠå¤–éƒ¨ç³»ç»Ÿæ•°æ®æ£€ç´¢ï¼Œå› æ­¤RAGçš„ `å“åº”æ—¶å»¶` ç›¸å¯¹è¾ƒé«˜ã€‚
2ï¼‰å¼•ç”¨çš„å¤–éƒ¨çŸ¥è¯†æ•°æ®ä¼š `æ¶ˆè€—å¤§é‡çš„æ¨¡å‹Token` èµ„æºã€‚

### 1.4 Retrievalæµç¨‹

![alt text](/public/langchain/retrieval/1.png)

**ç¯èŠ‚1:Source(æ•°æ®æº)**
æŒ‡çš„æ˜¯RAGæ¶æ„ä¸­æ‰€å¤–æŒ‚çš„çŸ¥è¯†åº“ã€‚è¿™é‡Œæœ‰ä¸‰ç‚¹è¯´æ˜ï¼š
1ã€åŸå§‹æ•°æ®æºç±»å‹å¤šæ ·ï¼šå¦‚ï¼šè§†é¢‘ã€å›¾ç‰‡ã€æ–‡æœ¬ã€ä»£ç ã€æ–‡æ¡£ç­‰
2ã€å½¢å¼å¤šæ ·æ€§ï¼š
- å¯ä»¥æ˜¯ä¸Šç™¾ä¸ª.csvæ–‡ä»¶ï¼Œå¯ä»¥æ˜¯åƒä¸‡ä¸ª.jsonæ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸Šä¸‡ä¸ª.pdfæ–‡ä»¶
- å¯ä»¥æ˜¯æŸä¸€ä¸ªä¸šåŠ¡æµç¨‹å¤–æ”¾çš„APIï¼Œå¯ä»¥æ˜¯æŸä¸ªç½‘ç«™çš„å®æ—¶æ•°æ®ç­‰

**ç¯èŠ‚2:Load(åŠ è½½)**

æ–‡æ¡£åŠ è½½å™¨ï¼ˆDocument Loadersï¼‰è´Ÿè´£å°†æ¥è‡ªä¸åŒæ•°æ®æºçš„éç»“æ„åŒ–æ–‡æœ¬ï¼ŒåŠ è½½åˆ°å†…å­˜ï¼Œæˆä¸ºæ–‡æ¡£ï¼ˆDocumentï¼‰å¯¹è±¡
æ–‡æ¡£å¯¹è±¡åŒ…å«`æ–‡æ¡£å†…å®¹`å’Œç›¸å…³`å…ƒæ•°æ®ä¿¡æ¯`ï¼Œä¾‹å¦‚TXTã€CSVã€HTMLã€JSONã€Markdownã€PDFï¼Œç”šè‡³YouTubeè§†é¢‘è½¬å½•ç­‰ã€‚

æ–‡æ¡£åŠ è½½å™¨è¿˜æ”¯æŒâ€œ**å»¶è¿ŸåŠ è½½**â€æ¨¡å¼ï¼Œä»¥ç¼“è§£å¤„ç†å¤§æ–‡ä»¶æ—¶çš„å†…å­˜å‹åŠ›ã€‚

**ç¯èŠ‚3ï¼šTransformï¼ˆè½¬æ¢ï¼‰**
**æ–‡æ¡£è½¬æ¢å™¨(Document Transformers)** è´Ÿè´£å¯¹åŠ è½½çš„æ–‡æ¡£è¿›è¡Œè½¬æ¢å’Œå¤„ç†ï¼Œä»¥ä¾¿æ›´å¥½åœ°é€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„éœ€æ±‚ã€‚
æ–‡æ¡£è½¬æ¢å™¨æä¾›äº†ä¸€è‡´çš„æ¥å£ï¼ˆå·¥å…·ï¼‰æ¥æ“ä½œæ–‡æ¡£ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ç±»ï¼š
- æ–‡æœ¬æ‹†åˆ†å™¨(Text Splitters) ï¼šå°†é•¿æ–‡æœ¬æ‹†åˆ†æˆè¯­ä¹‰ä¸Šç›¸å…³çš„å°å—ï¼Œä»¥é€‚åº”è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ã€‚
- å†—ä½™è¿‡æ»¤å™¨(Redundancy Filters) ï¼šè¯†åˆ«å¹¶è¿‡æ»¤é‡å¤çš„æ–‡æ¡£ã€‚
- å…ƒæ•°æ®æå–å™¨(Metadata Extractors) ï¼šä»æ–‡æ¡£ä¸­æå–æ ‡é¢˜ã€è¯­è°ƒç­‰ç»“æ„åŒ–å…ƒæ•°æ®ã€‚
- å¤šè¯­è¨€è½¬æ¢å™¨(Multi-lingual Transformers) ï¼šå®ç°æ–‡æ¡£çš„æœºå™¨ç¿»è¯‘ã€‚
- å¯¹è¯è½¬æ¢å™¨(Conversational Transformers) ï¼šå°†éç»“æ„åŒ–å¯¹è¯è½¬æ¢ä¸ºé—®ç­”æ ¼å¼çš„æ–‡æ¡£ã€‚
æ€»çš„æ¥è¯´ï¼Œæ–‡æ¡£è½¬æ¢å™¨æ˜¯ LangChain å¤„ç†ç®¡é“ä¸­éå¸¸é‡è¦çš„ä¸€ä¸ªç»„ä»¶ï¼Œå®ƒä¸°å¯Œäº†æ¡†æ¶å¯¹æ–‡æ¡£çš„è¡¨ç¤ºå’Œæ“ä½œèƒ½åŠ›ã€‚
åœ¨è¿™äº›åŠŸèƒ½ä¸­ï¼Œæ–‡æ¡£æ‹†åˆ†å™¨æ˜¯å¿…é¡»çš„æ“ä½œã€‚ä¸‹é¢å•ç‹¬è¯´æ˜ã€‚

**ç¯èŠ‚3.1ï¼šText Splittingï¼ˆæ–‡æ¡£æ‹†åˆ†ï¼‰**
- æ‹†åˆ†/åˆ†å—çš„å¿…è¦æ€§ ï¼šå‰ä¸€ä¸ªç¯èŠ‚åŠ è½½åçš„æ–‡æ¡£å¯¹è±¡å¯ä»¥ç›´æ¥ä¼ å…¥æ–‡æ¡£æ‹†åˆ†å™¨è¿›è¡Œæ‹†åˆ†ï¼Œè€Œæ–‡æ¡£åˆ‡å—åæ‰èƒ½ å‘é‡åŒ– å¹¶å­˜å…¥æ•°æ®åº“ä¸­ã€‚
- æ–‡æ¡£æ‹†åˆ†å™¨çš„å¤šæ ·æ€§ ï¼šLangChainæä¾›äº†ä¸°å¯Œçš„æ–‡æ¡£æ‹†åˆ†å™¨ï¼Œä¸ä»…èƒ½å¤Ÿåˆ‡åˆ†æ™®é€šæ–‡æœ¬ï¼Œè¿˜èƒ½åˆ‡åˆ†Markdownã€JSONã€HTMLã€ä»£ç ç­‰ç‰¹æ®Šæ ¼å¼çš„æ–‡æœ¬ã€‚
- æ‹†åˆ†/åˆ†å—çš„æŒ‘æˆ˜æ€§ ï¼šå®é™…æ‹†åˆ†æ“ä½œä¸­éœ€è¦å¤„ç†è®¸å¤šç»†èŠ‚é—®é¢˜ï¼Œ`ä¸åŒç±»å‹çš„æ–‡æœ¬`ã€ `ä¸åŒçš„ä½¿ç”¨åœºæ™¯`éƒ½éœ€è¦é‡‡ç”¨ä¸åŒçš„åˆ†å—ç­–ç•¥ã€‚
    - å¯ä»¥æŒ‰ç…§ `æ•°æ®ç±»å‹` è¿›è¡Œåˆ‡ç‰‡å¤„ç†ï¼Œæ¯”å¦‚é’ˆå¯¹ æ–‡æœ¬ç±»æ•°æ® ï¼Œå¯ä»¥ç›´æ¥æŒ‰ç…§å­—ç¬¦ã€æ®µè½è¿›è¡Œåˆ‡ç‰‡ï¼› `ä»£ç ç±»æ•°æ®` åˆ™éœ€è¦è¿›ä¸€æ­¥ç»†åˆ†ä»¥ä¿è¯ä»£ç çš„åŠŸèƒ½æ€§ï¼›
    - å¯ä»¥ç›´æ¥æ ¹æ® `token` è¿›è¡Œåˆ‡ç‰‡å¤„ç†

**åœ¨æ„å»ºRAGåº”ç”¨ç¨‹åºçš„æ•´ä¸ªæµç¨‹ä¸­ï¼Œæ‹†åˆ†/åˆ†å—æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„ç¯èŠ‚ä¹‹ä¸€ï¼Œå®ƒæ˜¾è‘—å½±å“æ£€ç´¢æ•ˆæœã€‚**ç›®å‰è¿˜æ²¡æœ‰é€šç”¨çš„æ–¹æ³•å¯ä»¥æ˜ç¡®æŒ‡å‡ºå“ªä¸€ç§åˆ†å—ç­–ç•¥æœ€ä¸ºæœ‰æ•ˆã€‚ä¸åŒçš„ä½¿ç”¨åœºæ™¯å’Œæ•°æ®ç±»å‹éƒ½ä¼šå½±å“åˆ†å—ç­–ç•¥çš„é€‰æ‹©ã€‚

**ç¯èŠ‚4ï¼šEmbedï¼ˆåµŒå…¥ï¼‰**

æ–‡æ¡£åµŒå…¥æ¨¡å‹ï¼ˆText Embedding Modelsï¼‰è´Ÿè´£å°† æ–‡æœ¬ è½¬æ¢ä¸º å‘é‡è¡¨ç¤º ï¼Œå³æ¨¡å‹èµ‹äºˆäº†æ–‡æœ¬è®¡ç®—æœºå¯
ç†è§£çš„æ•°å€¼è¡¨ç¤ºï¼Œä½¿æ–‡æœ¬å¯ç”¨äºå‘é‡ç©ºé—´ä¸­çš„å„ç§è¿ç®—ï¼Œå¤§å¤§æ‹“å±•äº†æ–‡æœ¬åˆ†æçš„å¯èƒ½æ€§ï¼Œæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸéå¸¸é‡è¦çš„æŠ€æœ¯ã€‚

![alt text](/public/langchain/retrieval/2.png)

æ–‡æœ¬åµŒå…¥ä¸º LangChain ä¸­çš„é—®ç­”ã€æ£€ç´¢ã€æ¨èç­‰åŠŸèƒ½æä¾›äº†é‡è¦æ”¯æŒã€‚å…·ä½“ä¸ºï¼š
- è¯­ä¹‰åŒ¹é… ï¼šé€šè¿‡è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œåˆ¤æ–­å®ƒä»¬åœ¨è¯­ä¹‰ä¸Šçš„ç›¸ä¼¼ç¨‹åº¦ï¼Œå®ç°è¯­ä¹‰åŒ¹é…ã€‚
- æ–‡æœ¬æ£€ç´¢ ï¼šé€šè¿‡è®¡ç®—ä¸åŒæ–‡æœ¬ä¹‹é—´çš„å‘é‡ç›¸ä¼¼åº¦ï¼Œå¯ä»¥å®ç°è¯­ä¹‰æœç´¢ï¼Œæ‰¾åˆ°å‘é‡ç©ºé—´ä¸­æœ€ç›¸ä¼¼çš„æ–‡æœ¬ã€‚
- ä¿¡æ¯æ¨è ï¼šæ ¹æ®ç”¨æˆ·çš„å†å²è®°å½•æˆ–å…´è¶£åµŒå…¥ç”Ÿæˆç”¨æˆ·å‘é‡ï¼Œè®¡ç®—ä¸åŒä¿¡æ¯çš„å‘é‡ä¸ç”¨æˆ·å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œæ¨èç›¸ä¼¼çš„ä¿¡æ¯ã€‚
- çŸ¥è¯†æŒ–æ˜ ï¼šå¯ä»¥é€šè¿‡èšç±»ã€é™ç»´ç­‰æ‰‹æ®µåˆ†ææ–‡æœ¬å‘é‡çš„åˆ†å¸ƒï¼Œå‘ç°æ–‡æœ¬ä¹‹é—´çš„æ½œåœ¨å…³è”ï¼ŒæŒ–æ˜çŸ¥è¯†ã€‚
- è‡ªç„¶è¯­è¨€å¤„ç† ï¼šå°†è¯è¯­ã€å¥å­ç­‰è¡¨ç¤ºä¸ºç¨ å¯†å‘é‡ï¼Œä¸ºç¥ç»ç½‘ç»œç­‰ä¸‹æ¸¸ä»»åŠ¡æä¾›è¾“å…¥ã€‚

**ç¯èŠ‚5ï¼šStoreï¼ˆå­˜å‚¨ï¼‰**
LangChain è¿˜æ”¯æŒæŠŠæ–‡æœ¬åµŒå…¥å­˜å‚¨åˆ°å‘é‡å­˜å‚¨æˆ–ä¸´æ—¶ç¼“å­˜ï¼Œä»¥é¿å…éœ€è¦é‡æ–°è®¡ç®—å®ƒä»¬ã€‚è¿™é‡Œå°±å‡ºç°äº†æ•°æ®åº“ï¼Œæ”¯æŒè¿™äº›åµŒå…¥çš„é«˜æ•ˆ `å­˜å‚¨` å’Œ `æœç´¢` çš„éœ€æ±‚ã€‚

![alt text](/public/langchain/retrieval/3.png)

**ç¯èŠ‚6ï¼šRetrieveï¼ˆæ£€ç´¢ï¼‰**
æ£€ç´¢å™¨ï¼ˆRetrieversï¼‰æ˜¯ä¸€ç§ç”¨äº`å“åº”éç»“æ„åŒ–æŸ¥è¯¢`çš„æ¥å£ï¼Œå®ƒå¯ä»¥è¿”å›ç¬¦åˆæŸ¥è¯¢è¦æ±‚çš„æ–‡æ¡£ã€‚
LangChain æä¾›äº†ä¸€äº›å¸¸ç”¨çš„æ£€ç´¢å™¨ï¼Œå¦‚`å‘é‡æ£€ç´¢å™¨`ã€`æ–‡æ¡£æ£€ç´¢å™¨` ã€`ç½‘ç«™ç ”ç©¶æ£€ç´¢å™¨`ç­‰ã€‚
é€šè¿‡é…ç½®ä¸åŒçš„æ£€ç´¢å™¨ï¼ŒLangChainå¯ä»¥çµæ´»åœ°å¹³è¡¡æ£€ç´¢çš„ç²¾åº¦ã€å¬å›ç‡ä¸æ•ˆç‡ã€‚æ£€ç´¢ç»“æœå°†ä¸ºåç»­çš„é—®ç­”ç”Ÿæˆæä¾›ä¿¡æ¯æ”¯æŒï¼Œä»¥äº§ç”Ÿæ›´åŠ å‡†ç¡®å’Œå®Œæ•´çš„å›ç­”ã€‚

## 2. æ–‡æ¡£åŠ è½½å™¨ (Document Loaders)
LangChainçš„è®¾è®¡ï¼šå¯¹äº Source ä¸­å¤šç§ä¸åŒçš„æ•°æ®æºï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ç§ç»Ÿä¸€çš„å½¢å¼è¯»å–ã€è°ƒç”¨ã€‚
### 2.1 åŠ è½½txt

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain.document_loaders import TextLoader
# 2.å®šä¹‰TextLoaderå¯¹è±¡ï¼Œfile_path=".txtçš„ä½ç½®"
text_loader = TextLoader(file_path="asset/load/01-langchain-utf-8.txt", encoding="utf-8")
# 3.åŠ è½½
docs = text_loader.load() #è¿”å›Liståˆ—è¡¨(Documentå¯¹è±¡)
# 4.æ‰“å°
print(docs)
```
```
[Document(metadata={'source':'asset/load/01-langchain-utf-8.txt'},page_content='LangChain'æ˜¯â¼€ä¸ªâ½¤äºæ„å»ºåŸºäºâ¼¤è¯­â¾”æ¨¡å‹ï¼ˆLLMï¼‰åº”â½¤çš„å¼€å‘æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ›´â¾¼æ•ˆåœ°é›†æˆã€ç®¡ç†å’Œå¢å¼ºâ¼¤è¯­â¾”æ¨¡å‹çš„èƒ½â¼’ï¼Œæ„å»ºç«¯åˆ°ç«¯çš„åº”â½¤ç¨‹åºã€‚å®ƒæä¾›äº†â¼€å¥—æ¨¡å—åŒ–â¼¯å…·å’Œæ¥å£ï¼Œâ½€æŒä»ç®€å•çš„â½‚æœ¬â½£æˆåˆ°å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡')]
```

Docummentå¯¹è±¡ä¸­æœ‰ä¸¤ä¸ªé‡è¦çš„å±æ€§ï¼š
- page_contentï¼šçœŸæ­£çš„æ–‡æ¡£å†…å®¹
- metadataï¼šæ–‡æ¡£å†…å®¹çš„åŸæ•°æ®

```python
type(docs[0]) #langchain_core.documents.base.Document
docs[0].page_content
'''
LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚LangChainç®€åŒ–äº†LLMåº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸçš„æ¯ä¸ªé˜¶æ®µã€‚\nLangChain å·²ç»æˆä¸ºäº†æˆ‘ä»¬æ¯ä¸€ä¸ªå¤§æ¨¡å‹å¼€å‘å·¥ç¨‹å¸ˆçš„æ ‡é…ã€‚
'''
docs[0].metadata # {'source': './data/langchain.txt'}
```

### 2.2 åŠ è½½pdf
**ä¸¾ä¾‹1ï¼š**
LangChainåŠ è½½PDFæ–‡ä»¶ä½¿ç”¨çš„æ˜¯pypdfï¼Œå…ˆå®‰è£…
> pip install pypdf

```python
# 1.å¯¼å…¥ç›¸å…³çš„ä¾èµ– PyPDFLoader()
from langchain.document_loaders import PyPDFLoader
# 2.å®šä¹‰PyPDFLoader
pdfLoader = PyPDFLoader(file_path="asset/load/02-load.pdf")
# 3.åŠ è½½
docs = pdfLoader.load()
print(docs)
print(type(docs[0]))
# # 4.éå†é›†åˆ
# for doc in docs:
# print(f"åŠ è½½çš„æ–‡æ¡£:{doc.page_content}")
```

### 2.3 åŠ è½½CSV
**ä¸¾ä¾‹1** ï¼šåŠ è½½csvæ‰€æœ‰åˆ—

```python
from langchain_community.document_loaders.csv_loader import CSVLoader
loader = CSVLoader(
    file_path="./asset/load/03-load.csv",
    source_column='author' # ä½¿ç”¨ source_column å‚æ•°æŒ‡å®šæ–‡ä»¶åŠ è½½çš„åˆ—ï¼Œä¿å­˜åœ¨sourceå˜é‡ä¸­ã€‚
)
data = loader.load()
print(data)
print(type(data)) # <class 'list'>
print(type(data[0])) # <class 'langchain_core.documents.base.Document'>
print(len(data)) # 4
print(data[0].page_content) # id: 1 title: Introduction to Python ...
```

### 2.4 åŠ è½½JSON
LangChainæä¾›çš„JSONæ ¼å¼çš„æ–‡æ¡£åŠ è½½å™¨æ˜¯ **JSONLoader** ã€‚åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­ï¼ŒJSONæ ¼å¼çš„æ•°æ®å æœ‰å¾ˆå¤§æ¯”ä¾‹ï¼Œè€Œä¸”JSONçš„å½¢å¼ä¹Ÿæ˜¯å¤šæ ·çš„ã€‚æˆ‘ä»¬éœ€è¦ç‰¹åˆ«å…³æ³¨ã€‚
JSONLoader ä½¿ç”¨æŒ‡å®šçš„ jqç»“æ„æ¥è§£æ JSON æ–‡ä»¶ã€‚jqæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å‘½ä»¤è¡Œ JSON å¤„ç†å™¨ ï¼Œå¯ä»¥å¯¹JSON æ ¼å¼çš„æ•°æ®è¿›è¡Œå„ç§å¤æ‚çš„å¤„ç†ï¼ŒåŒ…æ‹¬æ•°æ®è¿‡æ»¤ã€æ˜ å°„ã€å‡å°‘å’Œè½¬æ¢ï¼Œæ˜¯å¤„ç† JSON æ•°æ®çš„é¦–é€‰å·¥å…·ä¹‹ä¸€ã€‚
> pip install jq

**ä¸¾ä¾‹1** ï¼šä½¿ç”¨JSONLoaderæ–‡æ¡£åŠ è½½å™¨åŠ è½½

```python
# 1.å¯¼å…¥ä¾èµ–
from langchain_community.document_loaders import JSONLoader
from pprint import pprint
# 2.å®šä¹‰JSONLoaderå¯¹è±¡
# é”™è¯¯çš„
# json_loader=JSONLoader(file_path="asset/load/04-load.json")
# æƒ…å†µ1
# json_loader=JSONLoader(
# file_path="asset/load/04-load.json",
# jq_schema=".", #ç›´æ¥æå–å®Œæ•´çš„JSONå¯¹è±¡ï¼ˆåŒ…æ‹¬æ‰€æœ‰å­—æ®µï¼‰
# text_content=False #ä¿æŒåŸå§‹ JSON ç»“æ„ï¼Œå°†æå–çš„æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦å­—æ®µä¸­
# )ä¸²å­˜å…¥page_content

# æƒ…å†µ2
# .messages[].content:éå†.messages[]ä¸­æ‰€æœ‰å…ƒç´  ä»æ¯ä¸€ä¸ªå…ƒç´ ä¸­æå–.contentå­—æ®µ
json_loader=JSONLoader(
    file_path="asset/load/04-load.json",
    jq_schema=".messages[].content"
)
# 3.åŠ è½½
docs = json_pprint(docs)
loader.load()
# 4.æå–contentä¸­æŒ‡å®šå­—ç¬¦æ•°çš„å†…å®¹
# print(docs[0].page_content[:10])
```

**ä¸¾ä¾‹2** ï¼šæå–04-response.jsonæ–‡ä»¶ä¸­åµŒå¥—åœ¨ data.items[].content çš„æ–‡æœ¬
- å¦‚æœå¸Œæœ›å¤„ç† JSON ä¸­çš„ **åµŒå¥—å­—æ®µã€æ•°ç»„å…ƒç´ æå–**ï¼Œå¯ä»¥ä½¿ç”¨ content_key é…åˆis_content_key_jq_parsable=True ï¼Œé€šè¿‡ jq è¯­æ³•ç²¾å‡†å®šä½ç›®æ ‡æ•°æ®ã€‚
- é€šå¸¸ï¼Œå¯¹apiè¯·æ±‚ç»“æœçš„é‡‡é›†

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_community.document_loaders import JSONLoader
from pprint import pprint
# 2.å®šä¹‰jsonæ–‡ä»¶çš„è·¯å¾„
file_path = 'asset/load/04-response.json'
# 3.å®šä¹‰JSONLoaderå¯¹è±¡
# æå–åµŒå¥—åœ¨ data.items[].content çš„æ–‡æœ¬ï¼Œå¹¶ä¿ç•™å…¶ä»–å­—æ®µä½œä¸ºå…ƒæ•°æ®
# æ–¹å¼1ï¼š
# loader = JSONLoader(
# file_path=file_path,
# jq_schema=".data.items[].content",
# )
# æ–¹å¼2ï¼š
loader = JSONLoader(
file_path=file_path,
jq_schema=".data.items[]", # å…ˆå®šä½åˆ°æ•°ç»„æ¡ç›®
content_key=".content", # å†ä»æ¡ç›®ä¸­æå– content å­—æ®µ
is_content_key_jq_parsable=True # ç”¨jqè§£æcontent_key
)
# 4.åŠ è½½
data = loader.load()
pprint(data)
pprint(data[0].page_content)
```

### 2.5 åŠ è½½HTMLï¼ˆäº†è§£ï¼‰
> pip install unstructured

```python
# 1.å¯¼å…¥ç›¸å…³çš„ä¾èµ–
from langchain.document_loaders import UnstructuredHTMLLoader
# 2.å®šä¹‰UnstructuredHTMLLoaderå¯¹è±¡
# strategy:
# "fast" è§£æåŠ è½½htmlæ–‡ä»¶é€Ÿåº¦æ˜¯æ¯”è¾ƒå¿«ï¼ˆä½†å¯èƒ½ä¸¢å¤±éƒ¨åˆ†ç»“æ„æˆ–å…ƒæ•°æ®ï¼‰
# "hi_res": (é«˜åˆ†è¾¨ç‡è§£æ) è§£æç²¾å‡†ï¼ˆé€Ÿåº¦æ…¢ä¸€äº›ï¼‰
# "ocr_only" å¼ºåˆ¶ä½¿ç”¨ocræå–æ–‡æœ¬ï¼Œä»…ä»…é€‚ç”¨äºå›¾åƒï¼ˆå¯¹HTMLæ— æ•ˆï¼‰
# mode ï¼šone of `{'paged', 'elements', 'single'}
# "elements" æŒ‰è¯­ä¹‰å…ƒç´ ï¼ˆæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰ï¼‰æ‹†åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„å°æ–‡æ¡£
html_loader = UnstructuredHTMLLoader(
    file_path="asset/load/05-load.html",
    mode="elements",
    strategy="fast"
)
# 3.åŠ è½½
docs = html_loader.load()
print(len(docs)) # 16
# 4.æ‰“å°
for doc in docs:
    print(doc)
```

### 2.6 åŠ è½½Markdown(äº†è§£)
å°†Markdownæ–‡æ¡£æŒ‰è¯­ä¹‰å…ƒç´ ï¼ˆæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰ï¼‰æ‹†åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„å°æ–‡æ¡£ï¼ˆ Element å¯¹è±¡ï¼‰ï¼Œè€Œä¸æ˜¯è¿”å›å•ä¸ªå¤§æ–‡æ¡£ã€‚é€šè¿‡æŒ‡å®š `mode="elements"` è½»æ¾ä¿æŒè¿™ç§åˆ†ç¦»ã€‚æ¯ä¸ªåˆ†å‰²åçš„å…ƒç´ ä¼šåŒ…å«å…ƒæ•°æ®ã€‚

```python
# 1.å¯¼å…¥ç›¸å…³çš„ä¾èµ–
from langchain.document_loaders import UnstructuredMarkdownLoader
from pprint import pprint
# 2.å®šä¹‰UnstructuredMarkdownLoaderå¯¹è±¡
md_loader = UnstructuredMarkdownLoader(
    file_path="./asset/load/06-load.md",
    mode= "elements",
    strategy="fast"
)
# 3.åŠ è½½
docs = md_loader.load()
print(len(docs))
# 4.æ‰“å°
for doc in docs:
# pprint(doc)
pprint(doc.page_content)
```

### 2.7 åŠ è½½File Directory(äº†è§£)
é™¤äº†ä¸Šè¿°çš„å•ä¸ªæ–‡ä»¶åŠ è½½ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰¹é‡åŠ è½½ä¸€ä¸ªæ–‡ä»¶å¤¹å†…çš„æ‰€æœ‰æ–‡ä»¶ã€‚
> pip install unstructured

```python
# 1.å¯¼å…¥ç›¸å…³çš„ä¾èµ–
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import PythonLoader
from pprint import pprint
# 2.å®šä¹‰DirectoryLoaderå¯¹è±¡,æŒ‡å®šè¦åŠ è½½çš„æ–‡ä»¶å¤¹è·¯å¾„ã€è¦åŠ è½½çš„æ–‡ä»¶ç±»å‹å’Œæ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹
directory_loader = DirectoryLoader(
  path="./asset/load",
  glob="*.py",
  use_multithreading=True,
  show_progress=True,
  loader_cls=PythonLoader
)
# 3.åŠ è½½
docs = directory_loader.load()
# 4.æ‰“å°
print(len(docs))
for doc in docs:
  pprint(doc)
```

### 2.8 BaseLoaderã€Documentæºç åˆ†æ
ä¸€æ–¹é¢ï¼šLangChainåœ¨è®¾è®¡æ—¶ï¼Œè¦ä¿è¯Sourceä¸­å¤šç§ä¸åŒçš„æ•°æ®æºï¼Œåœ¨æ¥ä¸‹æ¥çš„æµç¨‹ä¸­å¯ä»¥ç”¨ä¸€ç§ç»Ÿä¸€çš„å½¢å¼è¯»å–ã€è°ƒç”¨ã€‚
å¦ä¸€æ–¹é¢ï¼šä¸ºä»€ä¹ˆ `PDFloader` å’Œ `TextLoader` ç­‰Document Loader éƒ½ä½¿ç”¨ load() å»åŠ è½½ï¼Œä¸”éƒ½ä½¿
ç”¨ `.page_content` å’Œ `.metadata` è¯»å–æ•°æ®ã€‚
ã€è§£ç­”ã€‘æ¯ä¸€ä¸ªåœ¨LangChainä¸­é›†æˆçš„æ–‡æ¡£åŠ è½½å™¨ï¼Œéƒ½è¦ç»§æ‰¿è‡ª **BaseLoader(æ–‡æ¡£åŠ è½½å™¨)** ï¼ŒBaseLoaderæä¾›äº†ä¸€ä¸ªåä¸º"load"çš„å…¬å¼€æ–¹æ³•ï¼Œç”¨äºä»é…ç½®çš„ä¸åŒ æ•°æ®æº åŠ è½½æ•°æ®ï¼Œå…¨éƒ¨ä½œä¸º`Document`å¯¹è±¡ã€‚å®ç°é€»è¾‘å¦‚ä¸‹æ‰€ç¤ºï¼š
**BaseLoaderç±»åˆ†æ**
BaseLoaderç±»å®šä¹‰äº†å¦‚ä½•ä»ä¸åŒçš„æ•°æ®æºåŠ è½½æ–‡æ¡£ï¼Œæ¯ä¸ªåŸºäºä¸åŒæ•°æ®æºå®ç°çš„loaderï¼Œéƒ½éœ€è¦ç»§æ‰¿**BaseLoader**ã€‚Baseloaderè¦æ±‚ä¸å¤šï¼Œå¯¹äºä»»ä½•å…·ä½“å®ç°çš„loaderï¼Œæœ€å°‘éƒ½è¦å®ç°loadæ–¹æ³•ã€‚

```python
class BaseLoader(ABC):
"""æ–‡æ¡£åŠ è½½å™¨æ¥å£ã€‚
å®ç°åº”å½“ä½¿ç”¨ç”Ÿæˆå™¨å®ç°å»¶è¿ŸåŠ è½½æ–¹æ³•ï¼Œä»¥é¿å…ä¸€æ¬¡æ€§å°†æ‰€æœ‰æ–‡æ¡£åŠ è½½è¿›å†…å­˜ã€‚
`load` æ–¹æ³•ä»…ä¾›ç”¨æˆ·æ–¹ä¾¿ä½¿ç”¨ï¼Œä¸åº”è¢«é‡å†™ã€‚
"""
# å­ç±»ä¸åº”ç›´æ¥å®ç°æ­¤æ–¹æ³•ã€‚è€Œåº”å®ç°å»¶è¿ŸåŠ è½½æ–¹æ³•ã€‚
def load(self) -> List[Document]:
"""å°†æ•°æ®åŠ è½½ä¸º Document å¯¹è±¡ã€‚"""
return list(self.lazy_load())
def load_and_split(
  self, text_splitter: Optional[TextSplitter] = None
) -> List[Document]:
"""åŠ è½½æ–‡æ¡£å¹¶å°†å…¶åˆ†å‰²æˆå—ã€‚å—ä»¥ Document å½¢å¼è¿”å›ã€‚
ä¸è¦é‡å†™æ­¤æ–¹æ³•ã€‚å®ƒåº”è¢«è§†ä¸ºå·²å¼ƒç”¨ï¼
å‚æ•°:
  text_splitter: ç”¨äºåˆ†å‰²æ–‡æ¡£çš„ TextSplitter å®ä¾‹ã€‚é»˜è®¤ä¸º RecursiveCharacterTextSplitterã€‚
è¿”å›:
  æ–‡æ¡£åˆ—è¡¨ã€‚
"""
.....
.....
  _text_splitter: TextSplitter = RecursiveCharacterTextSplitter()
else:
  _text_splitter = text_splitter
  docs = self.load()
  return _text_splitter.split_documents(docs)
```
`BaseLoader` æŠŠæ•°æ®åŠ è½½æˆ **Documents object** ï¼Œå­˜åˆ° `Documents` ç±»ä¸­çš„ `page_content` ä¸­ã€‚

**Documentç±»åˆ†æ**
`Document` å…è®¸ç”¨æˆ·ä¸æ–‡æ¡£çš„å†…å®¹è¿›è¡Œäº¤äº’ï¼Œå¯ä»¥æŸ¥çœ‹æ–‡æ¡£å†…å®¹ã€‚

```python
class Document(Serializable):
  ""ç”¨äºå­˜å‚¨æ–‡æœ¬åŠå…¶å…³è”å…ƒæ•°æ®çš„ç±»ã€‚"""
page_content: str
"""å­—ç¬¦ä¸²æ–‡æœ¬ã€‚"""
metadata: dict = Field(default_factory=dict)
"""å…³äºé¡µé¢å†…å®¹çš„ä»»æ„å…ƒæ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæ¥æºã€ä¸å…¶ä»–æ–‡æ¡£çš„å…³ç³»ç­‰ï¼‰ã€‚"""
type: Literal["Document"] = "Document"

def __init__(self, page_content: str,**kwargs: Any) -> None:
  """å°† page_content ä½œä¸ºä½ç½®å‚æ•°æˆ–å‘½åå‚æ•°ä¼ å…¥ã€‚"""
  super().__init__(page_content=page_content,**kwargs)

@classmethod
def is_lc_serializable(cls) -> bool:
  """è¿”å›æ­¤ç±»æ˜¯å¦å¯åºåˆ—åŒ–ã€‚"""
  return True

@classmethod
def get_lc_namespace(cls) -> List[str]:
  """è·å– langchain å¯¹è±¡çš„å‘½åç©ºé—´ã€‚"""
  return ["langchain", "schema", "document"]
```
é€šè¿‡ å­˜ + è¯»çš„ä¸¤ä¸ªåŸºç±»çš„æŠ½è±¡ï¼Œæ»¡è¶³ä¸åŒç±»å‹åŠ è½½å™¨åœ¨æ•°æ®å½¢å¼ä¸Šçš„ç»Ÿä¸€ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå…¶ä¸­çš„
`metadata`ä¼šæ ¹æ®loaderå®ç°çš„ä¸åŒå†™å…¥ä¸åŒçš„æ•°æ®ï¼ŒåŒæ ·æ˜¯ä¸€ä¸ªå¿…è¦çš„åŸºç¡€å±æ€§ã€‚

## 3.æ–‡æ¡£æ‹†åˆ†å™¨ (Text Splitters)
### 3.1 ä¸ºä»€ä¹ˆè¦æ‹†åˆ†/åˆ†å—/åˆ‡åˆ†
å½“æ‹¿åˆ°ç»Ÿä¸€çš„ä¸€ä¸ªDocumentå¯¹è±¡åï¼Œæ¥ä¸‹æ¥éœ€è¦åˆ‡åˆ†æˆChunksã€‚å¦‚æœä¸åˆ‡åˆ†ï¼Œè€Œæ˜¯è€ƒè™‘ä½œä¸ºä¸€ä¸ªæ•´ä½“çš„Documentå¯¹è±¡ï¼Œä¼šå­˜åœ¨ä¸¤ç‚¹é—®é¢˜ï¼š
> 1. å‡è®¾æé—®çš„Queryçš„ç­”æ¡ˆå‡ºç°åœ¨æŸä¸€ä¸ªDocumentå¯¹è±¡ä¸­ï¼Œé‚£ä¹ˆå°†æ£€ç´¢åˆ°çš„æ•´ä¸ªDocumentå¯¹è±¡ç›´æ¥æ”¾å…¥Promptä¸­å¹¶ ä¸æ˜¯æœ€ä¼˜çš„é€‰æ‹© ï¼Œå› ä¸ºå…¶ä¸­ä¸€å®šä¼š åŒ…å«éå¸¸å¤šæ— å…³çš„ä¿¡æ¯ ï¼Œè€Œæ— æ•ˆä¿¡æ¯è¶Šå¤šï¼Œå¯¹å¤§æ¨¡å‹åç»­çš„æ¨ç†å½±å“è¶Šå¤§ã€‚
> 2. ä»»ä½•ä¸€ä¸ªå¤§æ¨¡å‹éƒ½å­˜åœ¨æœ€å¤§è¾“å…¥çš„ Tokené™åˆ¶ ï¼Œå¦‚æœä¸€ä¸ªDocumentéå¸¸å¤§ï¼Œæ¯”å¦‚ä¸€ä¸ªå‡ ç™¾å…†çš„PDFï¼Œé‚£ä¹ˆå¤§æ¨¡å‹è‚¯å®šæ— æ³•å®¹çº³å¦‚æ­¤å¤šçš„ä¿¡æ¯ã€‚

åŸºäºæ­¤ï¼Œä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆå°±æ˜¯å°†å®Œæ•´çš„Documentå¯¹è±¡è¿›è¡Œ**åˆ†å—å¤„ç†ï¼ˆChunking)**ã€‚æ— è®ºæ˜¯åœ¨å­˜å‚¨è¿˜æ˜¯æ£€ç´¢è¿‡ç¨‹ä¸­ï¼Œéƒ½å°†ä»¥è¿™äº›**å—(chunks)**ä¸ºåŸºæœ¬å•ä½ï¼Œè¿™æ ·æœ‰æ•ˆåœ°é¿å…å†…å®¹ä¸ç›¸å…³æ€§é—®é¢˜å’Œè¶…å‡ºæœ€å¤§è¾“å…¥é™åˆ¶çš„é—®é¢˜ã€‚

### 3.2 Chunkingæ‹†åˆ†çš„ç­–ç•¥
æ–¹æ³•1ï¼šæ ¹æ®å¥å­åˆ‡åˆ†ï¼šè¿™ç§æ–¹æ³•æŒ‰ç…§è‡ªç„¶å¥å­è¾¹ç•Œè¿›è¡Œåˆ‡åˆ†ï¼Œä»¥ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚

æ–¹æ³•2ï¼šæŒ‰ç…§å›ºå®šå­—ç¬¦æ•°æ¥åˆ‡åˆ†ï¼šè¿™ç§ç­–ç•¥æ ¹æ®ç‰¹å®šçš„å­—ç¬¦æ•°é‡æ¥åˆ’åˆ†æ–‡æœ¬ï¼Œä½†å¯èƒ½ä¼šåœ¨ä¸é€‚å½“çš„ä½ç½®åˆ‡æ–­å¥å­ã€‚

æ–¹æ³•3ï¼šæŒ‰å›ºå®šå­—ç¬¦æ•°æ¥åˆ‡åˆ†ï¼Œç»“åˆé‡å çª—å£ï¼ˆoverlapping windowsï¼‰ï¼šæ­¤æ–¹æ³•ä¸æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†ç›¸ä¼¼ï¼Œä½†é€šè¿‡é‡å çª—å£æŠ€æœ¯é¿å…åˆ‡åˆ†å…³é”®å†…å®¹ï¼Œç¡®ä¿ä¿¡æ¯è¿è´¯æ€§ã€‚

æ–¹æ³•4ï¼šé€’å½’å­—ç¬¦åˆ‡åˆ†æ–¹æ³•ï¼šé€šè¿‡é€’å½’å­—ç¬¦æ–¹å¼åŠ¨æ€ç¡®å®šåˆ‡åˆ†ç‚¹ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥æ ¹æ®æ–‡æ¡£çš„å¤æ‚æ€§å’Œå†…å®¹å¯†åº¦æ¥è°ƒæ•´å—çš„å¤§å°ã€‚

æ–¹æ³•5ï¼šæ ¹æ®è¯­ä¹‰å†…å®¹åˆ‡åˆ†ï¼šè¿™ç§ é«˜çº§ç­–ç•¥ ä¾æ®æ–‡æœ¬çš„è¯­ä¹‰å†…å®¹æ¥åˆ’åˆ†å—ï¼Œæ—¨åœ¨ä¿æŒç›¸å…³ä¿¡æ¯çš„é›†ä¸­å’Œå®Œæ•´ï¼Œé€‚ç”¨äºéœ€è¦é«˜åº¦è¯­ä¹‰ä¿æŒçš„åº”ç”¨åœºæ™¯ã€‚

> ç¬¬2ç§â½…æ³•ï¼ˆæŒ‰ç…§å­—ç¬¦æ•°åˆ‡åˆ†ï¼‰å’Œç¬¬3ç§â½…æ³•ï¼ˆæŒ‰å›ºå®šå­—ç¬¦æ•°åˆ‡åˆ†ç»“åˆé‡å çª—å£ï¼‰ä¸»è¦åŸºäºå­—ç¬¦è¿›â¾â½‚æœ¬çš„åˆ‡åˆ†ï¼Œè€Œä¸è€ƒè™‘â½‚ç« çš„å®é™…å†…å®¹å’Œè¯­ä¹‰ã€‚è¿™ç§â½…å¼è™½ç®€å•ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´ `ä¸»é¢˜æˆ–è¯­ä¹‰ä¸Šçš„æ–­è£‚` ã€‚
ç›¸å¯¹è€Œâ¾”ï¼Œç¬¬4ç§é€’å½’â½…æ³•æ›´åŠ çµæ´»å’Œâ¾¼æ•ˆï¼Œå®ƒç»“åˆäº†å›ºå®šâ»“åº¦åˆ‡åˆ†å’Œè¯­ä¹‰åˆ†æã€‚é€šå¸¸æ˜¯ `é¦–é€‰ç­–`
ç•¥ ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ›´å¥½åœ°ç¡®ä¿æ¯ä¸ªæ®µè½åŒ…å«â¼€ä¸ªå®Œæ•´çš„ä¸»é¢˜ã€‚
è€Œç¬¬5ç§â½…æ³•ï¼ŒåŸºäºè¯­ä¹‰çš„åˆ†å‰²è™½ç„¶èƒ½ç²¾ç¡®åœ°åˆ‡åˆ†å‡ºå®Œæ•´çš„ä¸»é¢˜æ®µè½ï¼Œä½†è¿™ç§â½…æ³•æ•ˆç‡è¾ƒä½ã€‚å®ƒéœ€
è¦è¿â¾å¤æ‚çš„åˆ†æ®µç®—æ³•ï¼ˆsegmentation algorithmï¼‰ï¼Œ `å¤„ç†é€Ÿåº¦è¾ƒæ…¢` ï¼Œå¹¶ä¸” `æ®µè½é•¿åº¦å¯èƒ½æä¸å‡åŒ€` ï¼ˆæœ‰çš„ä¸»é¢˜æ®µè½å¯èƒ½å¾ˆâ»“ï¼Œè€Œæœ‰çš„åˆ™è¾ƒçŸ­ï¼‰ã€‚å› æ­¤ï¼Œå°½ç®¡å®ƒåœ¨æŸäº›éœ€è¦â¾¼ç²¾åº¦è¯­ä¹‰ä¿æŒçš„åœºæ™¯ä¸‹æœ‰å…¶åº”â½¤ä»·å€¼ï¼Œä½†å¹¶ `ä¸é€‚åˆæ‰€æœ‰æƒ…å†µ` ã€‚

è¿™äº›æ–¹æ³•å„æœ‰ä¼˜åŠ¿å’Œå±€é™ï¼Œé€‰æ‹©é€‚å½“çš„åˆ†å—ç­–ç•¥å–å†³äºå…·ä½“çš„åº”ç”¨éœ€æ±‚å’Œé¢„æœŸçš„æ£€ç´¢æ•ˆæœã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä¾æ¬¡å°è¯•ç”¨å¸¸è§„æ‰‹æ®µåº”è¯¥å¦‚ä½•å®ç°ä¸Šè¿°å‡ ç§æ–¹æ³•çš„æ–‡æœ¬åˆ‡åˆ†ã€‚

å°ç»“ï¼šå‡ ä¸ªå¸¸ç”¨çš„æ–‡æ¡£åˆ‡åˆ†å™¨çš„æ–¹æ³•çš„è°ƒç”¨

```python

#æ–¹å¼1ï¼šä¼ å…¥çš„å‚æ•°ç±»å‹ï¼šå­—ç¬¦ä¸²; è¿”å›å€¼ç±»å‹ï¼šList[str]
split_text(xxx)

#æ–¹å¼2ï¼šä¼ å…¥çš„å‚æ•°ç±»å‹ï¼šList[str]; è¿”å›å€¼ç±»å‹ï¼šList[Document]
create_documents(xxx) #åº•å±‚è°ƒç”¨äº†split_text(xxx)

#æ–¹å¼3ï¼šä¼ å…¥çš„å‚æ•°ç±»å‹ï¼šList[Document]; è¿”å›å€¼ç±»å‹ï¼šList[Document
split_documents(xxx) #åº•å±‚è°ƒç”¨äº†create_documents()
```
æ­¤å¤–ï¼Œè¿™é‡Œæä¾›äº†ä¸€ä¸ªå¯è§†åŒ–å±•ç¤ºæ–‡æœ¬å¦‚ä½•åˆ†å‰²çš„å·¥å…·ï¼Œhttps://chunkviz.up.railway.app/

### 3.3 å…·ä½“å®ç°
LangChainæä¾›äº†è®¸å¤šä¸åŒç±»å‹çš„æ–‡æ¡£åˆ‡åˆ†å™¨
å®˜ç½‘åœ°å€ï¼šhttps://python.langchain.com/api_reference/text_splitters/index.html
#### 3.3.1 CharacterTextSplitterï¼šSplit by character
å‚æ•°æƒ…å†µè¯´æ˜ï¼š
- `chunk_size` ï¼šæ¯ä¸ªåˆ‡å—çš„æœ€å¤§tokenæ•°é‡ï¼Œé»˜è®¤å€¼ä¸º4000ã€‚
- `chunk_overlap` ï¼šç›¸é‚»ä¸¤ä¸ªåˆ‡å—ä¹‹é—´çš„æœ€å¤§é‡å tokenæ•°é‡ï¼Œé»˜è®¤å€¼ä¸º200ã€‚
- `separator` ï¼šåˆ†å‰²ä½¿ç”¨çš„åˆ†éš”ç¬¦ï¼Œé»˜è®¤å€¼ä¸º"\n\n"ã€‚
- `length_function` ï¼šç”¨äºè®¡ç®—åˆ‡å—é•¿åº¦çš„æ–¹æ³•ã€‚é»˜è®¤èµ‹å€¼ä¸ºçˆ¶ç±»TextSplitterçš„lenå‡½æ•°ã€‚

**ä¸¾ä¾‹1ï¼š**
```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain.text_splitter import CharacterTextSplitter
# 2.å®šä¹‰è¦åˆ†å‰²çš„æ–‡æœ¬
text = "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬å•Šã€‚æˆ‘ä»¬å°†ä½¿ç”¨CharacterTextSplitterå°†å…¶åˆ†å‰²æˆå°å—ã€‚åˆ†å‰²åŸºäºå­—ç¬¦æ•°ã€‚"
# text = """
# LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹ã€‚é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶çš„ã€‚å®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’ŒæŠ½è±¡ã€‚ä½¿å¼€å‘è€…èƒ½å¤Ÿæ›´å®¹æ˜“åœ°æ„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºã€‚
# ""
# 3.å®šä¹‰åˆ†å‰²å™¨å®ä¾‹
text_splitter = CharacterTextSplitter(
  chunk_size=30, # æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
  chunk_overlap=5, # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
  separator="ã€‚", # æŒ‰å¥å·åˆ†å‰²
)
# 4.å¼€å§‹åˆ†å‰²
chunks = text_splitter.split_text(text)
# 5.æ‰“å°æ•ˆæœ
for i,chunk in enumerate(chunks):
  print(f"å—{i + 1}:é•¿åº¦ï¼š{len(chunk)}")
  print(chunk)
  print("-"*50)
```
**æ³¨æ„ï¼šæ— é‡å **

**separatorä¼˜å…ˆåŸåˆ™**ï¼šå½“è®¾ç½®äº† `separator` ï¼ˆå¦‚"ã€‚"ï¼‰ï¼Œåˆ†å‰²å™¨ä¼šé¦–å…ˆå°è¯•åœ¨åˆ†éš”ç¬¦å¤„åˆ†å‰²ï¼Œç„¶åå†è€ƒè™‘chunk_sizeã€‚è¿™æ˜¯ä¸ºäº†é¿å…åœ¨å¥å­ä¸­é—´ç¡¬æ€§åˆ‡æ–­ã€‚è¿™ç§è®¾è®¡æ˜¯ä¸ºäº†ï¼š

> 1. ä¼˜å…ˆä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼ˆä¸åˆ‡æ–­å¥å­ï¼‰
> 2. é¿å…äº§ç”Ÿæ— æ„ä¹‰çš„ç¢ç‰‡ï¼ˆå¦‚åŠä¸ªå•è¯/ä¸å®Œæ•´å¥å­ï¼‰
> 3. å¦‚æœ chunk_size æ¯”ç‰‡æ®µå°ï¼Œæ— æ³•æ‹†åˆ†ç‰‡æ®µï¼Œå¯¼è‡´ overlapå¤±æ•ˆã€‚
> 4. chunk_overlapä»…åœ¨åˆå¹¶åçš„ç‰‡æ®µä¹‹é—´ç”Ÿæ•ˆï¼ˆå¦‚æœ `chunk_size` è¶³å¤Ÿå¤§ï¼‰ã€‚å¦‚æœæ²¡æœ‰åˆå¹¶çš„ç‰‡æ®µï¼Œåˆ™ overlapå¤±æ•ˆã€‚è§ä¸¾ä¾‹3ã€‚

**ä¸¾ä¾‹2ï¼š**

**æ³¨æ„ï¼šæœ‰é‡å **

æ­¤æ—¶ï¼Œæ–‡æœ¬â€œè¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ã€‚â€çš„tokenæ­£å¥½å°±æ˜¯8ã€‚

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain.text_splitter import CharacterTextSplitter
# 2.å®šä¹‰è¦åˆ†å‰²çš„æ–‡æœ¬
text = "è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ã€‚è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ã€‚æœ€åä¸€æ®µç»“æŸã€‚"
# 3.å®šä¹‰å­—ç¬¦åˆ†å‰²å™¨
text_splitter = CharacterTextSplitter(
  separator="ã€‚",
  chunk_size=20,
  chunk_overlap=8,
  keep_
  separator=True #chunkä¸­æ˜¯å¦ä¿ç•™åˆ‡å‰²ç¬¦
)
# 4.åˆ†å‰²æ–‡æœ¬
chunks = text_splitter.split_text(text)
# 5.æ‰“å°ç»“æœ
for i,chunk in enumerate(chunks):
  print(f"å—{i + 1}:é•¿åº¦ï¼š{len(chunk)}")
  print(chunk)
  print("-"*50)
```

**3.3.2 RecursiveCharacterTextSplitterï¼šæœ€å¸¸ç”¨**

æ–‡æ¡£åˆ‡åˆ†å™¨ä¸­è¾ƒå¸¸ç”¨çš„æ˜¯`RecursiveCharacterTextSplitter`(é€’å½’å­—ç¬¦æ–‡æœ¬åˆ‡åˆ†å™¨) ï¼Œé‡åˆ°`ç‰¹å®šå­—ç¬¦`æ—¶è¿›è¡Œåˆ†å‰²ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå°è¯•è¿›è¡Œåˆ‡å‰²çš„å­—ç¬¦åŒ…æ‹¬ ["\n\n", "\n", " ", ""]ã€‚

å…·ä½“ä¸ºï¼šæ ¹æ®ç¬¬ä¸€ä¸ªå­—ç¬¦è¿›è¡Œåˆ‡å—ï¼Œä½†å¦‚æœä»»ä½•åˆ‡å—å¤ªå¤§ï¼Œåˆ™ä¼šç»§ç»­ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå­—ç¬¦ç»§ç»­åˆ‡å—ï¼Œä»¥æ­¤ç±»æ¨ã€‚
æ­¤å¤–ï¼Œè¿˜å¯ä»¥è€ƒè™‘æ·»åŠ ï¼Œã€‚ç­‰åˆ†å‰²å­—ç¬¦ã€‚

**ç‰¹ç‚¹ï¼š**
- **ä¿ç•™ä¸Šä¸‹æ–‡**ï¼šä¼˜å…ˆåœ¨è‡ªç„¶è¯­è¨€è¾¹ç•Œï¼ˆå¦‚æ®µè½ã€å¥å­ç»“å°¾ï¼‰å¤„åˆ†å‰²ï¼Œ`å‡å°‘ä¿¡æ¯ç¢ç‰‡åŒ–`ã€‚
- **æ™ºèƒ½åˆ†æ®µ**ï¼šé€šè¿‡é€’å½’å°è¯•å¤šç§åˆ†éš”ç¬¦ï¼Œå°†æ–‡æœ¬åˆ†å‰²ä¸ºå¤§å°æ¥è¿‘chunk_sizeçš„ç‰‡æ®µã€‚
- **çµæ´»é€‚é…**ï¼šé€‚ç”¨äºå¤šç§æ–‡æœ¬ç±»å‹ï¼ˆä»£ç ã€Markdownã€æ™®é€šæ–‡æœ¬ç­‰ï¼‰ï¼Œæ˜¯LangChainä¸­æœ€é€šç”¨çš„æ–‡æœ¬æ‹†åˆ†å™¨ã€‚

æ­¤å¤–ï¼Œè¿˜å¯ä»¥æŒ‡å®šçš„å‚æ•°åŒ…æ‹¬ï¼š
- **chunk_size** ï¼šåŒTextSplitterï¼ˆçˆ¶ç±»ï¼‰ã€‚
- **chunk_overlap** ï¼šåŒTextSplitterï¼ˆçˆ¶ç±»ï¼‰ã€‚
- **length_function** ï¼šåŒTextSplitterï¼ˆçˆ¶ç±»ï¼‰ã€‚
- **add_start_index** ï¼šåŒTextSplitterï¼ˆçˆ¶ç±»ï¼‰ã€‚

**ä¸¾ä¾‹1ï¼š** ä½¿ç”¨split_text()æ–¹æ³•æ¼”ç¤º

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain.text_splitter import RecursiveCharacterTextSplitter
# 2.å®šä¹‰RecursiveCharacterTextSplitteråˆ†å‰²å™¨å¯¹è±¡
text_splitter = RecursiveCharacterTextSplitter(
  chunk_size=10,
  chunk_overlap=0,
  add_start_index=True,
)
# 3.å®šä¹‰æ‹†åˆ†çš„å†…å®¹
text="LangChainæ¡†æ¶ç‰¹æ€§\n\nå¤šæ¨¡å‹é›†æˆ(GPT/Claude)\nè®°å¿†ç®¡ç†åŠŸèƒ½\né“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£åˆ†æåœºæ™¯ç¤ºä¾‹ï¼šéœ€è¦å¤„ç†PDF/Wordç­‰æ ¼å¼ã€‚"
# 4.æ‹†åˆ†å™¨åˆ†å‰²
paragraphs = text_splitter.split_text(text)
for para in paragraphs:
  print(para)
  print('-------')
```

**ä¸¾ä¾‹2ï¼š** ä½¿ç”¨create_documents()æ–¹æ³•æ¼”ç¤ºï¼Œä¼ å…¥å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¿”å›Documentå¯¹è±¡åˆ—è¡¨
```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain.text_splitter import RecursiveCharacterTextSplitter
# 2.å®šä¹‰RecursiveCharacterTextSplitteråˆ†å‰²å™¨å¯¹è±¡
text_splitter = RecursiveCharacterTextSplitter(
  chunk_size=10,
  chunk_overlap=0,
  add_start_index=True,
)
# 3.å®šä¹‰æ‹†åˆ†çš„å†…å®¹
list=["LangChainæ¡†æ¶ç‰¹æ€§\n\nå¤šæ¨¡å‹é›†æˆ(GPT/Claude)\nè®°å¿†ç®¡ç†åŠŸèƒ½\né“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£åˆ†æåœºæ™¯ç¤ºä¾‹ï¼šéœ€è¦å¤„ç†PDF/Wordç­‰æ ¼å¼ã€‚"]
# 4.æ‹†åˆ†å™¨åˆ†å‰²
# create_documents()ï¼šå½¢å‚æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¿”å›å€¼æ˜¯Documentçš„åˆ—è¡¨
paragraphs = text_splitter.create_documents(list)
for para in paragraphs:
  print(para)
  print('-------')
```

**ä¸¾ä¾‹3ï¼š** ä½¿ç”¨create_documents()æ–¹æ³•æ¼”ç¤ºï¼Œå°†æœ¬åœ°æ–‡ä»¶å†…å®¹åŠ è½½æˆå­—ç¬¦ä¸²ï¼Œè¿›è¡Œæ‹†åˆ†

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_text_splitters import RecursiveCharacterTextSplitter
# 2.æ‰“å¼€.txtæ–‡ä»¶
with open("asset/load/08-ai.txt", encoding="utf-8") as f:
  state_of_the_union = f.read() #è¿”å›çš„æ˜¯å­—ç¬¦ä¸²

# 3.å®šä¹‰RecursiveCharacterTextSplitterï¼ˆé€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼‰
text_splitter = RecursiveCharacterTextSplitter(
  chunk_size=100,
  chunk_overlap=20,
  #chunk_overlap=0,
  length_function=len
)
# 4.åˆ†å‰²æ–‡æœ¬
texts = text_splitter.create_documents([state_of_the_union])
# 5.æ‰“å°åˆ†å‰²æ–‡æœ¬
for text in texts:
  print(f"ğŸ”¥{text.page_content}")
```

**ä¸¾ä¾‹4:** ç”¨split_documents()æ–¹æ³•æ¼”ç¤ºï¼Œåˆ©ç”¨PDFLoaderåŠ è½½æ–‡æ¡£ï¼Œå¯¹æ–‡æ¡£çš„å†…å®¹ç”¨é€’å½’åˆ‡å‰²å™¨åˆ‡å‰²

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

# 2.å®šä¹‰PyPDFLoaderåŠ è½½å™¨
loader = PyPDFLoader("./asset/load/02-load.pdf")
# 3.åŠ è½½å’Œåˆ‡å‰²æ–‡æ¡£å¯¹è±¡
docs = loader.load() # è¿”å›Documentå¯¹è±¡æ„æˆçš„list
# print(f"ç¬¬0é¡µï¼š\n{docs[0]}")
# 4.å®šä¹‰åˆ‡å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
  chunk_size=200,
  #chunk_size=120,
  chunk_overlap=0,
  # chunk_overlap=100,
  length_function=len,
  add_start_index=True,
)
# 5.å¯¹pdfå†…å®¹è¿›è¡Œåˆ‡å‰²å¾—åˆ°æ–‡
æ¡£å¯¹è±¡
paragraphs = text_splitter.split_documents(docs)
#paragraphs = text_splitter.create_documents([text])
for para in paragraphs:
  print(para.page_content)
  print('-------')
```

**ä¸¾ä¾‹5ï¼š** è‡ªå®šä¹‰åˆ†éš”ç¬¦

æœ‰äº›ä¹¦å†™ç³»ç»Ÿæ²¡æœ‰å•è¯è¾¹ç•Œï¼Œä¾‹å¦‚ä¸­æ–‡ã€æ—¥æ–‡å’Œæ³°æ–‡ã€‚ä½¿ç”¨é»˜è®¤åˆ†éš”ç¬¦åˆ—è¡¨["\n\n", "\n", " ", ""]åˆ†å‰²æ–‡æœ¬å¯èƒ½å¯¼è‡´å•è¯é”™è¯¯çš„åˆ†å‰²ã€‚ä¸ºäº†ä¿æŒå•è¯åœ¨ä¸€èµ·ï¼Œä½ å¯ä»¥è‡ªå®šä¹‰åˆ†å‰²å­—ç¬¦ï¼Œè¦†ç›–åˆ†éš”ç¬¦åˆ—è¡¨ä»¥åŒ…å«é¢å¤–çš„æ ‡ç‚¹ç¬¦å·ã€‚

```python
text_splitter = RecursiveCharacterTextSplitter(
  chunk_size=200,
  chunk_overlap=20, # å¢åŠ é‡å å­—ç¬¦
  separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "â€¦â€¦", "ï¼Œ", ""], # æ·»åŠ ä¸­æ–‡æ ‡ç‚¹
  length_function=len,
  keep_separator=True #ä¿ç•™å¥å°¾æ ‡ç‚¹ï¼ˆå¦‚ â€¦â€¦ï¼‰ï¼Œé¿å…åˆ‡å‰²åä¸¢å¤±è¯­æ°”å’Œé€»è¾‘
)
```
æ•ˆæœï¼šç®—æ³•ä¼˜å…ˆåœ¨å¥å·ã€çœç•¥å·å¤„åˆ‡å‰²ï¼Œä¿æŒå¥å­å®Œæ•´æ€§ã€‚

#### 3.3.2 TokenTextSplitter/CharacterTextSplitterï¼šSplit by tokens
å½“æˆ‘ä»¬å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå—æ—¶ï¼Œé™¤äº†å­—ç¬¦ä»¥å¤–ï¼Œè¿˜å¯ä»¥ï¼š æŒ‰Tokençš„æ•°é‡åˆ†å‰² ï¼ˆè€Œéå­—ç¬¦æˆ–å•è¯æ•°ï¼‰ï¼Œå°†é•¿æ–‡æœ¬åˆ‡åˆ†æˆå¤šä¸ªå°å—ã€‚

**ä»€ä¹ˆæ˜¯Tokenï¼Ÿ**

- å¯¹æ¨¡å‹è€Œè¨€ï¼ŒTokenæ˜¯æ–‡æœ¬çš„æœ€å°å¤„ç†å•ä½ã€‚ä¾‹å¦‚ï¼š
  - è‹±æ–‡ï¼š"hello" â†’ 1ä¸ªTokenï¼Œ"ChatGPT" â†’ 2ä¸ªTokenï¼ˆ"Chat" + "GPT" ï¼‰ã€‚
  - ä¸­æ–‡ï¼š"äººå·¥æ™ºèƒ½" â†’ å¯èƒ½æ‹†åˆ†ä¸º2-3ä¸ªTokenï¼ˆå–å†³äºåˆ†è¯å™¨ï¼‰ã€‚

**ä¸ºä»€ä¹ˆæŒ‰Tokenåˆ†å‰²ï¼Ÿ**

- è¯­è¨€æ¨¡å‹å¯¹è¾“å…¥é•¿åº¦çš„é™åˆ¶æ˜¯åŸºäºTokenæ•°ï¼ˆå¦‚GPT-4çš„8k/32k Tokenä¸Šé™ï¼‰ï¼Œç›´æ¥æŒ‰å­—ç¬¦æˆ–å•è¯åˆ†å‰²å¯èƒ½å¯¼è‡´å®é™…Tokenæ•°è¶…é™ã€‚ï¼ˆç¡®ä¿æ¯ä¸ªæ–‡æœ¬å—ä¸è¶…è¿‡æ¨¡å‹çš„Tokenä¸Šé™ï¼‰
- å¤§è¯­è¨€æ¨¡å‹(LLM)é€šå¸¸æ˜¯ä»¥tokençš„æ•°é‡ä½œä¸ºå…¶è®¡é‡(æˆ–æ”¶è´¹)çš„ä¾æ®ï¼Œæ‰€ä»¥é‡‡ç”¨tokenåˆ†å‰²ä¹Ÿæœ‰åŠ©äºæˆ‘ä»¬åœ¨ä½¿ç”¨æ—¶æ›´æ–¹ä¾¿çš„æ§åˆ¶æˆæœ¬ã€‚

**TokenTextSplitter ä½¿ç”¨è¯´æ˜ï¼š**

- æ ¸å¿ƒä¾æ®ï¼šTokenæ•°é‡ + è‡ªç„¶è¾¹ç•Œã€‚ï¼ˆTokenTextSplitter ä¸¥æ ¼æŒ‰ç…§ token æ•°é‡è¿›è¡Œåˆ†å‰²ï¼Œä½†åŒæ—¶ä¼šä¼˜å…ˆåœ¨è‡ªç„¶è¾¹ç•Œï¼ˆå¦‚å¥å°¾ï¼‰å¤„åˆ‡æ–­ï¼Œä»¥å°½é‡ä¿è¯è¯­ä¹‰çš„å®Œæ•´æ€§ã€‚ï¼‰
- ä¼˜ç‚¹ï¼šä¸LLMçš„Tokenè®¡æ•°é€»è¾‘ä¸€è‡´ï¼Œèƒ½å°½é‡ä¿æŒè¯­ä¹‰å®Œæ•´
- ç¼ºç‚¹ï¼šå¯¹éè‹±è¯­æˆ–ç‰¹å®šé¢†åŸŸæ–‡æœ¬ï¼ŒTokenåŒ–æ•ˆæœå¯èƒ½ä¸ä½³
- å…¸å‹åœºæ™¯ï¼šéœ€è¦ç²¾ç¡®æ§åˆ¶Tokenæ•°è¾“å…¥LLMçš„åœºæ™¯

**ä¸¾ä¾‹1ï¼š** ä½¿ç”¨TokenTextSplitter

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_text_splitters import TokenTextSplitter
# 2.åˆå§‹åŒ– TokenTextSplitter
text_splitter = TokenTextSplitter(
  chunk_size=33, #æœ€å¤§ token æ•°ä¸º 32
  chunk_overlap=0, #é‡å  token æ•°ä¸º 0
  encoding_name="cl100k_base", # ä½¿ç”¨ OpenAI çš„ç¼–ç å™¨,å°†æ–‡æœ¬è½¬æ¢ä¸º token åºåˆ—
)
# 3.å®šä¹‰æ–‡æœ¬
text = "äººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€å‘æ¡†æ¶ã€‚å®ƒæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå·¥å…·é“¾ã€‚äººå·¥æ™ºèƒ½æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºç¨‹åºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ä¸€é—¨ç§‘å­¦ã€‚è‡ª20ä¸–çºª50å¹´ä»£è¯ç”Ÿä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚"

# 4.å¼€å§‹åˆ‡å‰²
texts = text_splitter.split_text(text)
# æ‰“å°åˆ†å‰²ç»“æœ
print(f"åŸå§‹æ–‡æœ¬è¢«åˆ†å‰²æˆäº† {len(texts)} ä¸ªå—:")
for i, chunk in enumerate(texts):
  print(f"å—{i+1}: é•¿åº¦ï¼š{len(chunk)} å†…å®¹ï¼š{chunk}")
  print("-" * 50)
```

**ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™æ ·çš„åˆ†å‰²ï¼Ÿ**

1ã€**`ç¬¬ä¸€å— (29å­—ç¬¦)`** ï¼šå†…å®¹æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¥å­ï¼Œä»¥å¥å·ç»“å°¾ã€‚TokenTextSplitterè¯†åˆ«åˆ°è¿™æ˜¯ä¸€ä¸ªè‡ªç„¶çš„è¯­ä¹‰è¾¹ç•Œï¼Œå³ä½¿è¿™é‡Œçš„ token æ•°é‡å¯èƒ½å°šæœªè¾¾åˆ° 33ï¼Œå®ƒä¹Ÿé€‰æ‹©åœ¨æ­¤å¤„åˆ‡å‰²ï¼Œä»¥ä¿è¯ç¬¬ä¸€å—è¯­ä¹‰çš„å®Œæ•´æ€§ã€‚

2ã€**`ç¬¬äºŒå— (32å­—ç¬¦)`** ï¼šå†…å®¹åŒ…å«äº†å¦ä¸€ä¸ªå®Œæ•´å¥å­ **`â€œäººå·¥æ™ºèƒ½æ˜¯æŒ‡...ä¸€é—¨ç§‘å­¦ã€‚â€`** ä»¥åŠä¸‹ä¸€å¥çš„å¼€å¤´ â€œè‡ª20ä¸–çºª50â€ ã€‚åˆ†å‰²å™¨åœ¨å¤„ç†å®Œç¬¬ä¸€ä¸ªå¥å­çš„ token åï¼Œå¯èƒ½ token æ•°é‡å·²ç»æ¥è¿‘ **`chunk_size`** ï¼Œäºæ˜¯åœ¨ä¸‹ä¸€ä¸ªè‡ªç„¶è¾¹ç•Œï¼ˆè¿™é‡Œæ˜¯å¥å·ï¼‰ä¹‹åç»§ç»­è¯»å–äº†å°‘é‡ tokenï¼ˆâ€œè‡ª20ä¸–çºª50â€ï¼‰ï¼Œç›´åˆ°éå¸¸æ¥è¿‘ 33token çš„é™åˆ¶ã€‚

**æ³¨æ„**ï¼šâ€œ50â€ ä¹‹åè¢«åˆ‡æ–­ï¼Œæ˜¯å› ä¸ºç¼–ç å™¨å¾ˆå¯èƒ½å°†â€œ50â€è¯†åˆ«ä¸ºä¸€ä¸ªç‹¬ç«‹çš„ tokenï¼Œè€Œâ€œå¹´ä»£â€æ˜¯å¦ä¸€ä¸ª tokenã€‚ä¸ºäº†ä¿è¯ token çš„å®Œæ•´æ€§ï¼Œå®ƒä¸ä¼šåœ¨â€œ50â€å­—ç¬¦ä¸­é—´åˆ‡æ–­ã€‚

3ã€**`ç¬¬ä¸‰å— (19å­—ç¬¦)`** ï¼šæ˜¯ç¬¬äºŒå—ä¸­æ–­å†…å®¹çš„å‰©ä½™éƒ¨åˆ†ï¼Œå½¢æˆäº†ä¸€ä¸ªè¾ƒçŸ­çš„å—ã€‚è¿™æ˜¯å› ä¸ºå‰©ä½™å†…å®¹æœ¬èº«çš„ token æ•°é‡å°±è¾ƒå°‘ã€‚

ç‰¹åˆ«æ³¨æ„ï¼š**å­—ç¬¦é•¿åº¦ä¸ç­‰äº Token æ•°é‡ã€‚**

**ä¸¾ä¾‹2**ï¼šä½¿ç”¨CharacterTextSplitter

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_text_splitters import CharacterTextSplitter
import tiktoken # ç”¨äºè®¡ç®—Tokenæ•°é‡

# 2.å®šä¹‰é€šè¿‡Tokenåˆ‡å‰²å™¨
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
  encoding_name="cl100k_base", # ä½¿ç”¨ OpenAI çš„ç¼–ç å™¨
  chunk_size=18,
  chunk_overlap=0,
  separator="ã€‚", # æŒ‡å®šä¸­æ–‡å¥å·ä¸ºåˆ†éš”ç¬¦
  keep_separator=False, # chunkä¸­æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦
)
# 3.å®šä¹‰æ–‡æœ¬
text = "äººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€å‘æ¡†æ¶ã€‚å®ƒæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå·¥å…·é“¾ã€‚ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæƒ³å‡ºå»è¸é’ã€‚ä½†æ˜¯åˆæ¯”è¾ƒæ‡’ä¸æƒ³å‡ºå»ï¼Œæ€ä¹ˆåŠ"

# 4.å¼€å§‹åˆ‡å‰²
texts = text_splitter.split_text(text)
print(f"åˆ†å‰²åçš„å—æ•°: {len(texts)}")
# 5.åˆå§‹åŒ–tiktokenç¼–ç å™¨ï¼ˆç”¨äºTokenè®¡æ•°ï¼‰
encoder = tiktoken.get_encoding("cl100k_base") # ç¡®ä¿CharacterTextSplitterçš„encoding_nameä¸€è‡´
# 6.æ‰“å°æ¯ä¸ªå—çš„Tokenæ•°å’Œå†…å®¹
for i, chunk in enumerate(texts):
  tokens = encoder.encode(chunk) # ç°åœ¨encoderå·²å®šä¹‰
  print(f"å—{i + 1}: {len(tokens)} Token\nå†…å®¹: {chunk}\n")
```

#### 3.3.3 SemanticChunkerï¼šè¯­ä¹‰åˆ†å—
SemanticChunkingï¼ˆè¯­ä¹‰åˆ†å—ï¼‰æ˜¯ LangChain ä¸­ä¸€ç§æ›´é«˜çº§çš„æ–‡æœ¬åˆ†å‰²æ–¹æ³•ï¼Œå®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºå­—ç¬¦æˆ–å›ºå®šå¤§å°çš„åˆ†å—æ–¹å¼ï¼Œè€Œæ˜¯æ ¹æ® æ–‡æœ¬çš„è¯­ä¹‰ç»“æ„ è¿›è¡Œæ™ºèƒ½åˆ†å—ï¼Œä½¿æ¯ä¸ªåˆ†å—ä¿æŒ è¯­ä¹‰å®Œæ•´æ€§ ï¼Œä»è€Œæé«˜æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç­‰åº”ç”¨çš„æ•ˆæœã€‚

**è¯­ä¹‰åˆ†å‰² vs ä¼ ç»Ÿåˆ†å‰²**

|ç‰¹æ€§| è¯­ä¹‰åˆ†å‰²ï¼ˆSemanticChunkerï¼‰| ä¼ ç»Ÿå­—ç¬¦åˆ†å‰²ï¼ˆRecursiveCharacterï¼‰|
|:---:|:---:|:---:|
|**åˆ†å‰²ä¾æ®** |åµŒå…¥å‘é‡ç›¸ä¼¼åº¦| å›ºå®šå­—ç¬¦/æ¢è¡Œç¬¦|
|**è¯­ä¹‰å®Œæ•´æ€§** | âœ… ä¿æŒä¸»é¢˜è¿è´¯ | âŒ å¯èƒ½åˆ‡æ–­å¥å­é€»è¾‘ |
|**è®¡ç®—æˆæœ¬** | âŒ é«˜ï¼ˆéœ€åµŒå…¥æ¨¡å‹ï¼‰| âœ… ä½ |
|**é€‚ç”¨åœºæ™¯** | éœ€è¦é«˜è¯­ä¹‰ä¸€è‡´æ€§çš„ä»»åŠ¡ | ç®€å•æ–‡æœ¬é¢„å¤„ç† |

**ä¸¾ä¾‹ï¼š**

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings
import os
import dotenv

dotenv.load_dotenv()
# åŠ è½½æ–‡æœ¬
with open("asset/load/09-ai1.txt", encoding="utf-8") as f:
  state_of_the_union = f.read() #è¿”å›å­—ç¬¦ä¸²

# è·å–åµŒå…¥æ¨¡å‹
os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")
embed_model = OpenAIEmbeddings(
  model="text-embedding-3-large"
)
# è·å–åˆ‡å‰²å™¨
text_splitter = SemanticChunker(
  embeddings=embed_model,
  breakpoint_threshold_type="percentile",#æ–­ç‚¹é˜ˆå€¼ç±»å‹ï¼šå­—é¢å€¼["ç™¾åˆ†ä½æ•°", "æ ‡å·®", "å››åˆ†ä½å‡†è·", "æ¢¯åº¦"] é€‰å…¶ä¸€
  breakpoint_threshold_amount=65.0 #æ–­ç‚¹é˜ˆå€¼æ•°é‡ (æä½é˜ˆå€¼ â†’ é«˜åˆ†å‰²æ•æ„Ÿåº¦)
)
# åˆ‡åˆ†æ–‡æ¡£
docs = text_splitter.create_documents(texts = [state_of_the_union])
print(len(docs))
for doc in docs:
  print(f"ğŸ” æ–‡æ¡£ {doc}:")
```

**å…³äºå‚æ•°çš„è¯´æ˜ï¼š**

> 1. breakpoint_threshold_type ï¼ˆæ–­ç‚¹é˜ˆå€¼ç±»å‹ï¼‰
- **ä½œç”¨**ï¼šå®šä¹‰æ–‡æœ¬è¯­ä¹‰è¾¹ç•Œçš„æ£€æµ‹ç®—æ³•ï¼Œå†³å®šä½•æ—¶åˆ†å‰²æ–‡æœ¬å—ã€‚
- å¯é€‰å€¼åŠåŸç†ï¼š

|ç±»å‹ |åŸç†è¯´æ˜| é€‚ç”¨åœºæ™¯|
|:---:|:----:|:----:|
|**`percentile`**|è®¡ç®—ç›¸é‚»å¥å­åµŒå…¥å‘é‡çš„ä½™å¼¦è·ç¦»ï¼Œå–**è·ç¦»åˆ†å¸ƒçš„ç¬¬Nç™¾åˆ†ä½å€¼**ä½œä¸ºé˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼åˆ™åˆ†å‰²|å¸¸è§„æ–‡æœ¬ï¼ˆå¦‚æ–‡ç« ã€æŠ¥å‘Šï¼‰|
|**`standard_deviation`**|ä»¥**å‡å€¼ + Nå€æ ‡å‡†å·®**ä¸ºé˜ˆå€¼ï¼Œè¯†åˆ«è¯­ä¹‰çªå˜ç‚¹|è¯­ä¹‰å˜åŒ–å‰§çƒˆçš„æ–‡æ¡£ï¼ˆå¦‚æŠ€æœ¯æ‰‹å†Œï¼‰|
|**`interquartile`**| ç”¨**å››åˆ†ä½è·ï¼ˆIQRï¼‰** å®šä¹‰å¼‚å¸¸å€¼è¾¹ç•Œï¼Œè¶…è¿‡åˆ™åˆ†å‰²|é•¿æ–‡æ¡£ï¼ˆå¦‚ä¹¦ç±ï¼‰|
|**`gradient`**|åŸºäº**åµŒå…¥å‘é‡å˜åŒ–çš„æ¢¯åº¦**æ£€æµ‹åˆ†å‰²ç‚¹ï¼ˆéœ€è‡ªå®šä¹‰å®ç°ï¼‰| å®éªŒæ€§éœ€æ±‚ |

> 2. breakpoint_threshold_amount ï¼ˆæ–­ç‚¹é˜ˆå€¼é‡ï¼‰

- **ä½œç”¨**ï¼šæ§åˆ¶åˆ†å‰²çš„**ç²’åº¦æ•æ„Ÿåº¦**ï¼Œå€¼è¶Šå°åˆ†å‰²è¶Šç»†ï¼ˆå—è¶Šå¤šï¼‰ï¼Œå€¼è¶Šå¤§åˆ†å‰²è¶Šç²—ï¼ˆå—è¶Šå°‘ï¼‰ã€‚
- å–å€¼èŒƒå›´ä¸ç¤ºä¾‹ï¼š
  - **`percentile`** æ¨¡å¼ï¼š0.0~100.0ï¼Œç”¨æˆ·ä»£ç è®¾ 65.0 è¡¨ç¤ºä»…å½“ä½™å¼¦è·ç¦» > æ‰€æœ‰è·ç¦»ä¸­æœ€ä½çš„65.0%å€¼æ—¶åˆ†å‰² ã€‚é»˜è®¤å€¼æ˜¯ï¼š95.0ï¼Œå…¼é¡¾è¯­ä¹‰å®Œæ•´æ€§ä¸æ£€ç´¢æ•ˆç‡ã€‚å€¼è¿‡å°ï¼ˆæ¯”å¦‚0.1ï¼‰ï¼Œä¼šäº§ç”Ÿå¤§é‡å°æ–‡æœ¬å—ï¼Œè¿‡åº¦åˆ†å‰²å¯èƒ½å¯¼è‡´ä¸Šä¸‹æ–‡æ–­è£‚ã€‚
  - **`standard_deviation`** æ¨¡å¼ï¼šæµ®ç‚¹æ•°ï¼ˆå¦‚ **1.5** è¡¨ç¤ºå‡å€¼+1.5å€æ ‡å‡†å·®ï¼‰ã€‚
  - **`interquartile`** æ¨¡å¼ï¼šå€æ•°ï¼ˆå¦‚ **1.5** æ˜¯IQRæ ‡å‡†å€¼ï¼‰ã€‚

#### 3.3.4 å…¶å®ƒæ‹†åˆ†å™¨
**ç±»å‹1ï¼šHTMLHeaderTextSplitterï¼šSplit by HTML header**

HTMLHeaderTextSplitteræ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå¤„ç†HTMLæ–‡æ¡£çš„æ–‡æœ¬åˆ†å‰²æ–¹æ³•ï¼Œå®ƒæ ¹æ®HTMLçš„ æ ‡é¢˜æ ‡ç­¾ï¼ˆå¦‚\<h1\>ã€\<h2\>ç­‰ï¼‰ å°†æ–‡æ¡£åˆ’åˆ†ä¸ºé€»è¾‘åˆ†å—ï¼ŒåŒæ—¶ä¿ç•™æ ‡é¢˜çš„å±‚çº§ç»“æ„ä¿¡æ¯ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain.text_splitter import HTMLHeaderTextSplitter
# 2.å®šä¹‰HTMLæ–‡ä»¶
html_string = """
<!DOCTYPE html>
<html>
<body>
  <div>
    <h1>æ¬¢è¿æ¥åˆ°å°šç¡…è°·ï¼</h1>
    <p>å°šç¡…è°·æ˜¯ä¸“é—¨åŸ¹è®­ITæŠ€æœ¯æ–¹å‘</p>
    <div>
      <h2>å°šç¡…è°·è€å¸ˆç®€ä»‹</h2>
      <p>å°šç¡…è°·è€å¸ˆæ‹¥æœ‰å¤šå¹´æ•™å­¦ç»éªŒï¼Œéƒ½æ˜¯ä»ä¸€çº¿äº’è”ç½‘ä¸‹æ¥</p>
      <h3>å°šç¡…è°·åŒ—äº¬æ ¡åŒº</h3>
      <p>åŒ—äº¬æ ¡åŒºä½äºå®ç¦ç§‘æŠ€å›­åŒº</p>
    </div>
  </div>
</body>
</html>
"""

# 4.ç”¨äºæŒ‡å®šè¦æ ¹æ®å“ªäº›HTMLæ ‡ç­¾æ¥åˆ†å‰²æ–‡æœ¬
headers_to_split_on = [
  ("h1", "æ ‡é¢˜1"),
  ("h2", "æ ‡é¢˜2"),
  ("h3", "æ ‡é¢˜3"),
]
# 5.å®šä¹‰HTMLHeaderTextSplitteråˆ†å‰²å™¨
html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
# 6.åˆ†å‰²å™¨åˆ†å‰²
html_header_splits = html_splitter.split_text(html_string)
html_header_splits
```

è¯´æ˜ï¼š
- æ ‡é¢˜ä¸‹æ–‡æœ¬å†…å®¹æ‰€å±æ ‡é¢˜çš„å±‚çº§ä¿¡æ¯ä¿å­˜åœ¨å…ƒæ•°æ®ä¸­ã€‚
- æ¯ä¸ªåˆ†å—ä¼šè‡ªåŠ¨ç»§æ‰¿çˆ¶çº§æ ‡é¢˜çš„ä¸Šä¸‹æ–‡ï¼Œé¿å…ä¿¡æ¯å‰²è£‚ã€‚

**ç±»å‹2ï¼šCodeTextSplitterï¼šSplit code**

CodeTextSplitteræ˜¯ä¸€ä¸ª ä¸“ä¸ºä»£ç æ–‡ä»¶ è®¾è®¡çš„æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆText Splitterï¼‰ï¼Œæ”¯æŒä»£ç çš„è¯­è¨€åŒ…æ‹¬['cpp','go', 'java', 'js', 'php', 'proto', 'python', 'rst', 'ruby', 'rust', 'scala', 'swift', 'markdown', 'latex', 'html','sol']ã€‚å®ƒèƒ½å¤Ÿæ ¹æ®ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ç»“æ„ï¼ˆå¦‚å‡½æ•°ã€ç±»ã€ä»£ç å—ç­‰ï¼‰æ™ºèƒ½åœ°æ‹†åˆ†ä»£ç ï¼Œä¿æŒä»£ç é€»è¾‘çš„å®Œæ•´æ€§ã€‚

ä¸é€’å½’æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆå¦‚RecursiveCharacterTextSplitterï¼‰ä¸åŒï¼ŒCodeTextSplitter é’ˆå¯¹ä»£ç çš„ç‰¹æ€§è¿›è¡Œäº†ä¼˜åŒ–ï¼Œ**`é¿å…åœ¨å‡½æ•°æˆ–ç±»çš„ä¸­é—´æˆªæ–­`**ã€‚

**ä¸¾ä¾‹**ï¼šæ”¯æŒçš„è¯­è¨€

> pip install langchain-text-splitters

```python
from langchain.text_splitter import Language
# æ”¯æŒåˆ†å‰²è¯­è¨€ç±»å‹
# Full list of supported languages
langs = [e.value for e in Language]
print(langs)
```

**ç±»å‹3ï¼šMarkdownTextSplitterï¼šmdæ•°æ®ç±»å‹**

å› ä¸ºMarkdownæ ¼å¼æœ‰ç‰¹å®šçš„è¯­æ³•ï¼Œä¸€èˆ¬æ•´ä½“å†…å®¹ç”± **h1ã€h2ã€h3** ç­‰å¤šçº§æ ‡é¢˜ç»„ç»‡ï¼Œæ‰€ä»¥
MarkdownHeaderTextSplitterçš„åˆ‡åˆ†ç­–ç•¥å°±æ˜¯æ ¹æ® **`æ ‡é¢˜æ¥åˆ†å‰²æ–‡æœ¬å†…å®¹`**ã€‚

**ä¸¾ä¾‹ï¼š**

```python
from langchain.text_splitter import MarkdownTextSplitter
markdown_text = """
# ä¸€çº§æ ‡é¢˜\n
è¿™æ˜¯ä¸€çº§æ ‡é¢˜ä¸‹çš„å†…å®¹\n\n
## äºŒçº§æ ‡é¢˜\n
- äºŒçº§ä¸‹åˆ—è¡¨é¡¹1\n
- äºŒçº§ä¸‹åˆ—è¡¨é¡¹2\n
"""

# å…³é”®æ­¥éª¤ï¼šç›´æ¥ä¿®æ”¹å®ä¾‹å±æ€§
splitter = MarkdownTextSplitter(chunk_size=30, chunk_overlap=0)
splitter.is_separator_regex = True # å¼ºåˆ¶å°†åˆ†éš”ç¬¦è§†ä¸ºæ­£åˆ™è¡¨è¾¾å¼
# æ‰§è¡Œåˆ†å‰²
docs = splitter.create_documents(texts = [markdown_text])
# print(len(docs))
for i, doc in enumerate(docs):
  print(f"\nğŸ” åˆ†å—{i + 1}:")
  print(doc.page_content)
```

## 4. æ–‡æ¡£åµŒå…¥æ¨¡å‹ (Text Embedding Models)
### 4.1 åµŒå…¥æ¨¡å‹æ¦‚è¿°
**Text Embedding Models**ï¼šæ–‡æ¡£åµŒå…¥æ¨¡å‹ï¼Œæä¾›å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡çš„èƒ½åŠ›ï¼Œå³ **`æ–‡æ¡£å‘é‡åŒ–`** ã€‚ `æ–‡æ¡£å†™å…¥` å’Œ `ç”¨æˆ·æŸ¥è¯¢åŒ¹é…` å‰éƒ½ä¼šå…ˆæ‰§è¡Œæ–‡æ¡£åµŒå…¥ç¼–ç ï¼Œå³å‘é‡åŒ–ã€‚

![alt text](/public/langchain/retrieval/4.png)

- LangChainæä¾›äº† è¶…è¿‡25ç§ ä¸åŒçš„åµŒå…¥æä¾›å•†å’Œæ–¹æ³•çš„é›†æˆï¼Œä»å¼€æºåˆ°ä¸“æœ‰APIï¼Œæ€»æœ‰ä¸€æ¬¾é€‚åˆä½ ã€‚
- Hugging Faceç­‰å¼€æºç¤¾åŒºæä¾›äº†ä¸€äº›æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹ï¼ˆä¾‹å¦‚BGEï¼‰ï¼Œæ•ˆæœæ¯”é—­æºä¸”è°ƒç”¨APIçš„å‘é‡åŒ–æ¨¡å‹æ•ˆæœå¥½ï¼Œå¹¶ä¸”å‘é‡åŒ–æ¨¡å‹å‚æ•°é‡å°ï¼Œåœ¨CPUä¸Šå³å¯è¿è¡Œã€‚æ‰€ä»¥ï¼Œè¿™é‡Œæ¨èåœ¨å¼€å‘RAGåº”ç”¨çš„è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ å¼€æºçš„æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹ ã€‚æ­¤å¤–ï¼Œå¼€æºæ¨¡å‹è¿˜å¯ä»¥æ ¹æ®åº”ç”¨åœºæ™¯ä¸‹æ”¶é›†çš„æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜æ¨¡å‹æ•ˆæœã€‚

LangChainä¸­é’ˆå¯¹å‘é‡åŒ–æ¨¡å‹çš„å°è£…æä¾›äº†ä¸¤ç§æ¥å£ï¼Œä¸€ç§é’ˆå¯¹ **`æ–‡æ¡£çš„å‘é‡åŒ–(embed_documents)`** ï¼Œä¸€ç§é’ˆå¯¹ **`å¥å­çš„å‘é‡åŒ–embed_query`**ã€‚

### 4.2 å¥å­çš„å‘é‡åŒ–ï¼ˆembed_queryï¼‰

**ä¸¾ä¾‹ï¼š**

```python
from langchain
import os
import dotenv_openai import OpenAIEmbeddings
dotenv.load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")
# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embeddings_model = OpenAIEmbeddings(model="text-embedding-ada-002")
#embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large")
# å¾…åµŒå…¥çš„æ–‡æœ¬å¥å­
text = "What was the name mentioned in the conversation?"
# ç”Ÿæˆä¸€ä¸ªåµŒå…¥å‘é‡
embedded_query = embeddings_model.embed_query(text = text)
# ä½¿ç”¨embedded_query[:5]æ¥æŸ¥çœ‹å‰5ä¸ªå…ƒç´ çš„å€¼
print(embedded_query[:5])
print(len(embedded_query))
```
### 4.3 æ–‡æ¡£çš„å‘é‡åŒ–ï¼ˆembed_documentsï¼‰
æ–‡æ¡£çš„å‘é‡åŒ–ï¼Œæ¥æ”¶çš„å‚æ•°æ˜¯å­—ç¬¦ä¸²æ•°ç»„ã€‚

**ä¸¾ä¾‹1ï¼š**
```python
from langchain_openai import OpenAIEmbeddings
import numpy as np
import pandas as pd
import os
import dotenv
dotenv.load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")
# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embeddings_model = OpenAIEmbeddings(model="text-embedding-ada-002")
# å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
texts = [
  "Hi there!",
  "Oh, hello!",
  "What's your name?",
  "My friends call me World",
  "Hello World!"
]
# ç”ŸæˆåµŒå…¥å‘é‡
embeddings = embeddings_model.embed_documents(texts)
for i in range(len(texts)):
  print(f"{texts[i]}:{embeddings[i][:3]}",end="\n\n")
```

**ä¸¾ä¾‹2ï¼š**
```python
from dotenv import load_dotenv
from langchain_community.document_loaders import CSVLoader
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings(
  model="text-embedding-3-large",
)
# æƒ…å†µ1ï¼š
loader = CSVLoader("./asset/load/03-load.csv", encoding="utf-8")
docs = loader.load_and_split()
#print(len(docs))
# å­˜æ”¾çš„æ˜¯æ¯ä¸€ä¸ªchrunkçš„embedding
embeded_docs = embeddings_model.embed_documents([doc.page_content for doc in docs])
print(len(embeded_docs))
# è¡¨ç¤ºçš„æ˜¯æ¯ä¸€ä¸ªchrunkçš„embeddingçš„ç»´åº¦
print(len(embeded_docs[0]))
print(embeded_docs[0][:10])
```

## 5. å‘é‡å­˜å‚¨(Vector Stores)
### 5.1 ç†è§£å‘é‡å­˜å‚¨
å°†æ–‡æœ¬å‘é‡åŒ–ä¹‹åï¼Œä¸‹ä¸€æ­¥å°±æ˜¯è¿›è¡Œå‘é‡çš„å­˜å‚¨ã€‚è¿™éƒ¨åˆ†åŒ…å«ä¸¤å—ï¼š
- `å‘é‡çš„å­˜å‚¨` ï¼šå°†éç»“æ„åŒ–æ•°æ®å‘é‡åŒ–åï¼Œå®Œæˆå­˜å‚¨
- `å‘é‡çš„æŸ¥è¯¢` ï¼šæŸ¥è¯¢æ—¶ï¼ŒåµŒå…¥éç»“æ„åŒ–æŸ¥è¯¢å¹¶æ£€ç´¢ä¸åµŒå…¥æŸ¥è¯¢â€œæœ€ç›¸ä¼¼â€çš„åµŒå…¥å‘é‡ã€‚å³å…·æœ‰ç›¸ä¼¼æ€§æ£€ç´¢èƒ½åŠ›

![alt text](/public/langchain/retrieval/5.png)

![alt text](/public/langchain/retrieval/6.png)

### 5.2 å¸¸ç”¨çš„å‘é‡æ•°æ®åº“

Langchainæä¾›äº†è¶…è¿‡50ç§ä¸åŒå‘é‡å­˜å‚¨(Vetor Stores)çš„é›†æˆï¼Œä»å¼€æºçš„æœ¬åœ°å‘é‡å­˜å‚¨åˆ°äº‘æ‰˜ç®¡çš„ç§æœ‰å‘é‡å­˜å‚¨ï¼Œå…è®¸ä½ é€‰æ‹©æœ€é€‚åˆéœ€æ±‚çš„å‘é‡å­˜å‚¨ã€‚

LangChainæ”¯æŒçš„å‘é‡å­˜å‚¨å‚è€ƒ `VetorStore` æ¥å£å’Œå®ç°ã€‚

å…¸å‹çš„ä»‹ç»å¦‚ä¸‹ï¼š
|å‘é‡æ•°æ®åº“|æè¿°|
|:---:|:---:|
|Chroma|å¼€æºã€å…è´¹çš„åµŒå…¥å¼æ•°æ®åº“|
|FAISS|Metaå‡ºå“ã€å¼€æºã€å…è´¹ã€Facebook AIç›¸ä¼¼æ€§æœç´¢æœåŠ¡ã€‚(Facebook AI Similarity Search,Facebook AIç›¸ä¼¼æ€§æœç´¢åº“)|
|Milvus|ç”¨äºå­˜å‚¨ã€ç´¢å¼•å’Œç®¡ç†ç”±æ·±åº¦ç¥ç»ç½‘ç»œå’Œå…¶ä»–MLæ¨¡å‹äº§ç”Ÿçš„å¤§é‡åµŒå…¥å‘é‡çš„æ•°æ®åº“|
|Pinecone|ç”¨äºå¹¿æ³›åŠŸèƒ½çš„å‘é‡æ•°æ®åº“|
|Redis|åŸºäºRedisçš„æ£€ç´¢å™¨|

### 5.3 å‘é‡æ•°æ®åº“çš„ç†è§£
å‡è®¾ä½ æ˜¯ä¸€åæ‘„å½±å¸ˆï¼Œæ‹äº†å¤§é‡çš„ç…§ç‰‡ã€‚ä¸ºäº†æ–¹ä¾¿ç®¡ç†å’ŒæŸ¥æ‰¾ï¼Œä½ å†³å®šå°†è¿™äº› ç…§ç‰‡å­˜å‚¨ åˆ°ä¸€ä¸ªæ•°æ®åº“ä¸­ã€‚ä¼ ç»Ÿçš„ å…³ç³»å‹æ•°æ®åº“ ï¼ˆå¦‚ MySQLã€PostgreSQL ç­‰ï¼‰å¯ä»¥å¸®åŠ©ä½  å­˜å‚¨ç…§ç‰‡çš„å…ƒæ•°æ® ï¼Œæ¯”å¦‚æ‹æ‘„æ—¶é—´ã€åœ°ç‚¹ã€ç›¸æœºå‹å·ç­‰ã€‚

ä½†æ˜¯ï¼Œå½“ä½ æƒ³è¦æ ¹æ® `ç…§ç‰‡çš„å†…å®¹ï¼ˆå¦‚é¢œè‰²ã€çº¹ç†ã€ç‰©ä½“ç­‰ï¼‰` è¿›è¡Œæœç´¢æ—¶ï¼Œä¼ ç»Ÿæ•°æ®åº“å°†æ— æ³•æ»¡è¶³ä½ çš„éœ€æ±‚ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä»¥æ•°æ®è¡¨çš„å½¢å¼å­˜å‚¨æ•°æ®ï¼Œå¹¶ä½¿ç”¨æŸ¥è¯¢è¯­å¥è¿›è¡Œç²¾ç¡®æœç´¢ã€‚é‚£ä¹ˆæ­¤æ—¶ï¼Œå‘é‡æ•°æ®åº“å°±å¯ä»¥æ´¾ä¸Šç”¨åœºã€‚

æˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªå¤šç»´çš„ç©ºé—´ä½¿å¾—æ¯å¼ ç…§ç‰‡ç‰¹å¾éƒ½å­˜åœ¨äºè¿™ä¸ªç©ºé—´å†…ï¼Œå¹¶ç”¨å·²æœ‰çš„ç»´åº¦è¿›è¡Œè¡¨ç¤ºï¼Œæ¯”å¦‚æ—¶é—´ã€åœ°ç‚¹ã€ç›¸æœºå‹å·ã€é¢œè‰²....æ­¤ç…§ç‰‡çš„ä¿¡æ¯å°†ä½œä¸ºä¸€ä¸ªç‚¹ï¼Œå­˜å‚¨äºå…¶ä¸­ã€‚ä»¥æ­¤ç±»æ¨ï¼Œå³å¯åœ¨è¯¥ç©ºé—´ä¸­æ„å»ºå‡ºæ— æ•°çš„ç‚¹ï¼Œè€Œåæˆ‘ä»¬å°†è¿™äº›ç‚¹ä¸ç©ºé—´åæ ‡è½´çš„åŸç‚¹ç›¸è¿æ¥ï¼Œå°±æˆä¸ºäº†ä¸€æ¡æ¡å‘é‡ï¼Œå½“è¿™äº›ç‚¹å˜ä¸ºå‘é‡ä¹‹åï¼Œå³å¯åˆ©ç”¨å‘é‡çš„è®¡ç®—è¿›ä¸€æ­¥è·å–æ›´å¤šçš„ä¿¡æ¯ã€‚å½“è¦è¿›è¡Œç…§ç‰‡çš„æ£€ç´¢æ—¶ï¼Œä¹Ÿä¼šå˜å¾—æ›´å®¹æ˜“æ›´å¿«æ·ã€‚

**æ³¨æ„**ï¼šåœ¨å‘é‡æ•°æ®åº“ä¸­è¿›è¡Œæ£€ç´¢æ—¶ï¼Œæ£€ç´¢å¹¶ `ä¸æ˜¯å”¯ä¸€çš„ã€ç²¾ç¡®çš„` ï¼Œè€Œæ˜¯æŸ¥è¯¢å’Œç›®æ ‡å‘é‡ æœ€ä¸ºç›¸ä¼¼çš„ä¸€äº›å‘é‡ ï¼Œå…·æœ‰æ¨¡ç³Šæ€§ã€‚

**å»¶ä¼¸æ€è€ƒï¼š** åªè¦å¯¹å›¾ç‰‡ã€è§†é¢‘ã€å•†å“ç­‰ç´ æè¿›è¡Œå‘é‡åŒ–ï¼Œå°±å¯ä»¥å®ç°ä»¥å›¾æœå›¾ã€è§†é¢‘ç›¸å…³æ¨èã€ç›¸ä¼¼å®è´æ¨èç­‰åŠŸèƒ½ã€‚

### 5.4 ä»£ç å®ç°
ä½¿ç”¨å‘é‡æ•°æ®åº“ç»„ä»¶æ—¶éœ€è¦åŒæ—¶ä¼ å…¥åŒ…å« æ–‡æœ¬å—çš„Documentç±»å¯¹è±¡ ä»¥åŠ `æ–‡æœ¬å‘é‡åŒ–ç»„ä»¶` ï¼Œå‘é‡æ•°æ®åº“ç»„ä»¶ä¼šè‡ªåŠ¨å®Œæˆå°†æ–‡æœ¬å‘é‡åŒ–çš„å·¥ä½œï¼Œå¹¶å†™å…¥æ•°æ®åº“ä¸­ã€‚

#### 5.4.1 æ•°æ®çš„å­˜å‚¨

**ä¸¾ä¾‹**ï¼šä»TXTæ–‡æ¡£ä¸­åŠ è½½æ•°æ®ï¼Œå‘é‡åŒ–åå­˜å‚¨åˆ°Chromaæ•°æ®åº“

å®‰è£…æ¨¡å—ï¼š
> pip install chromadb
> 
> pip install langchain-chroma

```python
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import Chroma_community.document
from langchain_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
# ä¸¾ä¾‹ï¼šå°†åˆ†å‰²åçš„æ–‡æœ¬ï¼Œä½¿ç”¨ OpenAI åµŒå…¥æ¨¡å‹è·å–åµŒå…¥å‘é‡ï¼Œå¹¶å­˜å‚¨åœ¨ Chroma ä¸­
# è·å–åµŒå…¥æ¨¡å‹
my_embedding = OpenAIEmbeddings(model="text-embedding-ada-002")
# åˆ›å»ºTextLoaderå®ä¾‹ï¼Œå¹¶åŠ è½½æŒ‡å®šçš„æ–‡æ¡£
loader = TextLoader("./asset/load/09-ai1.txt", encoding='utf-8')
documents = loader.load()
# åˆ›å»ºæ–‡æœ¬æ‹†åˆ†å™¨
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
# æ‹†åˆ†æ–‡æ¡£
docs = text_splitter.split_documents(documents)

# å­˜å‚¨ï¼šå°†æ–‡æ¡£å’Œæ•°æ®å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­
db = Chroma.from_documents(docs, my_embedding)
# æŸ¥è¯¢ï¼šä½¿ç”¨ç›¸ä¼¼åº¦æŸ¥æ‰¾
query = "äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯éƒ½æœ‰å•¥ï¼Ÿ"
docs = db.similarity_search(query)
print(docs[0].page_content)
```

**æ€è€ƒï¼šæ­¤æ—¶æ•°æ®å­˜å‚¨åœ¨å“ªé‡Œå‘¢ï¼Ÿ**

**æ³¨æ„**ï¼šChromaä¸»è¦æœ‰ä¸¤ç§å­˜å‚¨æ¨¡å¼ï¼š `å†…å­˜æ¨¡å¼` å’Œ `æŒä¹…åŒ–æ¨¡å¼` ã€‚å½“ä½¿ç”¨persist_directoryå‚æ•°æ—¶ï¼Œæ•°æ®ä¼šä¿å­˜åˆ°æŒ‡å®šç›®å½•ï¼›å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œåˆ™é»˜è®¤ä½¿ç”¨å†…å­˜å­˜å‚¨ã€‚

#### 5.4.2 æ•°æ®çš„æ£€ç´¢
ä¸¾ä¾‹ï¼šä¸€ä¸ªåŒ…å«æ„å»ºChromaå‘é‡æ•°æ®åº“ä»¥åŠå‘é‡æ£€ç´¢çš„ä»£ç 
å‰ç½®ä»£ç ï¼š
```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
# 2.å®šä¹‰æ–‡æ¡£
raw_documents = [
  Document(
    page_content="è‘¡è„æ˜¯ä¸€ç§å¸¸è§çš„æ°´æœï¼Œå±äºè‘¡è„ç§‘è‘¡è„å±æ¤ç‰©ã€‚å®ƒçš„æœå®å‘ˆåœ†å½¢æˆ–æ¤­åœ†å½¢ï¼Œé¢œè‰²æœ‰ç»¿è‰²ã€ç´«è‰²ã€çº¢è‰²ç­‰å¤šç§ã€‚è‘¡è„å¯Œå«ç»´ç”Ÿç´ Cå’ŒæŠ—æ°§åŒ–ç‰©è´¨ï¼Œå¯ä»¥ç›´æ¥é£Ÿç”¨æˆ–é…¿é€ æˆè‘¡è„é…’ã€‚",
    metadata={"source": "æ°´æœ", "type": "æ¤ç‰©"}
  ),
  Document(
    page_content="ç™½èœæ˜¯åå­—èŠ±ç§‘è”¬èœï¼ŒåŸäº§äºä¸­å›½åŒ—æ–¹ã€‚å®ƒçš„å¶ç‰‡å½¢æˆç´§å¯†çš„çƒçŠ¶å±‚å±‚åŒ…è£¹ï¼Œå£æ„Ÿæ¸…è„†å¾®ç”œã€‚ç™½èœå¯Œå«è†³é£Ÿçº¤ç»´å’Œç»´ç”Ÿç´ Kï¼Œå¸¸ç”¨äºåˆ¶ä½œæ³¡èœã€ç‚’èœæˆ–ç…®æ±¤ã€‚",
    metadata={"source": "è”¬èœ", "type": "æ¤ç‰©"}
  ),
  Document(
    page_content="ç‹—æ˜¯äººç±»æœ€æ—©é©¯åŒ–çš„åŠ¨ç‰©ä¹‹ä¸€ï¼Œå±äºçŠ¬ç§‘ã€‚å®ƒä»¬å…·æœ‰é«˜åº¦ç¤¾ä¼šæ€§ï¼Œèƒ½ç†è§£äººç±»æƒ…ç»ªï¼Œå¸¸è¢«ç”¨ä½œå® ç‰©ã€å¯¼ç›²çŠ¬æˆ–è­¦çŠ¬ã€‚ä¸åŒå“ç§çš„ç‹—åœ¨ä½“å‹ã€æ¯›è‰²å’Œæ€§æ ¼ä¸Šæœ‰å¾ˆå¤§å·®å¼‚ã€‚",
    metadata={"source": "åŠ¨ç‰©", "type": "å“ºä¹³åŠ¨ç‰©"}
  )

  # 3. åˆ›å»ºåµŒå…¥æ¨¡å‹
embedding = OpenAIEmbeddings(model="text-embedding-ada-002")
# 4.åˆ›å»ºå‘é‡æ•°æ®åº“
db = Chroma.from_documents(
  documents=raw_documents,
  embedding=embedding,
  persist_directory="./asset/chroma-3",
)
```

**â‘  ç›¸ä¼¼æ€§æ£€ç´¢ï¼ˆsimilarity_searchï¼‰**

æ¥æ”¶å­—ç¬¦ä¸²ä½œä¸ºå‚æ•°ï¼š

```python
# 5. æ£€ç´¢ç¤ºä¾‹ï¼ˆè¿”å›å‰3ä¸ªæœ€ç›¸å…³ç»“æœï¼‰
query = "å“ºä¹³åŠ¨ç‰©"
docs = db.similarity_
search(query, k=3) # k=3è¡¨ç¤ºè¿”å›3ä¸ªæœ€ç›¸å…³æ–‡
print(f"æŸ¥è¯¢: '{query}' çš„ç»“æœ:")
for i, doc in enumerate(docs, 1):
  print(f"\nç»“æœ {i}:")
  print(f"å†…å®¹: {doc.page_content}")
  print(f"å…ƒæ•°æ®: {doc.metadata}")
```

**â‘¡ æ”¯æŒç›´æ¥å¯¹é—®é¢˜å‘é‡æŸ¥è¯¢ï¼ˆsimilarity_search_by_vectorï¼‰**

æœç´¢ä¸ç»™å®šåµŒå…¥å‘é‡ç›¸ä¼¼çš„æ–‡æ¡£ï¼Œå®ƒæ¥å—`åµŒå…¥å‘é‡ä½œä¸ºå‚æ•°`ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²ã€‚

```python
query = "å“ºä¹³åŠ¨ç‰©"
embedding_vector = embedding.embed_query(query)

docs = db.similarity_search_by_vector(embedding_vector, k=3)

print(f"æŸ¥è¯¢: '{query}' çš„ç»“æœ:")
for i, doc in enumerate(docs, 1):
  print(f"\nç»“æœ {i}:")
  print(f"å†…å®¹: {doc.page_content}")
  print(f"å…ƒæ•°æ®: {doc.metadata}")
```

**â‘¢ ç›¸ä¼¼æ€§æ£€ç´¢ï¼Œæ”¯æŒè¿‡æ»¤å…ƒæ•°æ®ï¼ˆfilterï¼‰**

```python
query = "å“ºä¹³åŠ¨ç‰©"
docs = db.similarity_search(query=query,k=3,filter={"source": "åŠ¨ç‰©"})
for i, doc in enumerate(docs, 1):
  print(f"\nç»“æœ {i}:")
  print(f"å†…å®¹: {doc.page_content}")
  print(f"å…ƒæ•°æ®: {doc.metadata}")
```

**â‘£ é€šè¿‡L2è·ç¦»åˆ†æ•°è¿›è¡Œæœç´¢ï¼ˆsimilarity_search_with_scoreï¼‰**

è¯´æ˜ï¼šåˆ†æ•°å€¼è¶Šå°ï¼Œæ£€ç´¢åˆ°çš„æ–‡æ¡£è¶Šå’Œé—®é¢˜ç›¸ä¼¼ã€‚åˆ†å€¼å–å€¼èŒƒå›´ï¼š[0ï¼Œæ­£æ— ç©·]

```python

docs = db.similarity_search_with_score(
  "é‡å­åŠ›å­¦æ˜¯ä»€ä¹ˆ?"
)
for doc, score in docs:
  print(f" [L2è·ç¦»å¾—åˆ†={score:.3f}]{doc.page_content} [{doc.metadata}]")
```

**â‘¤ é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ†æ•°è¿›è¡Œæœç´¢ï¼ˆ_similarity_search_with_relevance_scoresï¼‰**

è¯´æ˜ï¼šåˆ†æ•°å€¼è¶Šæ¥è¿‘1ï¼ˆä¸Šé™ï¼‰ï¼Œæ£€ç´¢åˆ°çš„æ–‡æ¡£è¶Šå’Œé—®é¢˜ç›¸ä¼¼ã€‚

```python
docs = db._similarity_search_with_relevance_scores(
  "é‡å­åŠ›å­¦æ˜¯ä»€ä¹ˆ?"
)
for doc, score in docs:
  print(f"* [ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†={score:.3f}]{doc.page_content} [{doc.metadata}]")
```

**â‘¥ MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼Œmax_marginal_relevance_searchï¼‰**

MMR æ˜¯ä¸€ç§å¹³è¡¡ `ç›¸å…³æ€§` å’Œ `å¤šæ ·æ€§` çš„æ£€ç´¢ç­–ç•¥ï¼Œé¿å…è¿”å›é«˜åº¦ç›¸ä¼¼çš„å†—ä½™ç»“æœã€‚

```python
docs = db.max_marginal_relevance_search(
  query="é‡å­åŠ›å­¦æ˜¯ä»€ä¹ˆ",
  lambda_mult=0.8, # ä¾§é‡ç›¸ä¼¼æ€§
)
print("ğŸ” å…³äºã€é‡å­åŠ›å­¦æ˜¯ä»€ä¹ˆã€‘çš„æœç´¢ç»“æœï¼š")
print("=" * 50)
for i, doc in enumerate(docs):
  print(f"\nğŸ“– ç»“æœ {i+1}:")
  print(f"ğŸ“Œ å†…å®¹: {doc.page_content}")
  print(f"ğŸ· æ ‡ç­¾: {', '.join(f'{k}={v}' for k, v in doc.metadata.items())}")
```

å‚æ•°è¯´æ˜ï¼š `lambda_mult` å‚æ•°å€¼ä»‹äº 0 åˆ° 1 ä¹‹é—´ï¼Œç”¨äºç¡®å®šç»“æœä¹‹é—´çš„å¤šæ ·æ€§ç¨‹åº¦ï¼Œå…¶ä¸­ 0 å¯¹åº”æœ€å¤§å¤šæ ·æ€§ï¼Œ1 å¯¹åº”æœ€å°å¤šæ ·æ€§ã€‚é»˜è®¤å€¼ä¸º 0.5ã€‚

## 6. æ£€ç´¢å™¨(å¬å›å™¨) Retrievers
### 6.1 ä»‹ç»

ä»â€œå‘é‡å­˜å‚¨ç»„ä»¶â€çš„ä»£ç å®ç°5.4.2ä¸­å¯ä»¥çœ‹åˆ°ï¼Œå‘é‡æ•°æ®åº“æœ¬èº«å·²ç»åŒ…å«äº†å®ç°å¬å›åŠŸèƒ½çš„å‡½æ•°æ–¹æ³•(`similarity_search`)ã€‚è¯¥å‡½æ•°é€šè¿‡è®¡ç®—åŸå§‹æŸ¥è¯¢å‘é‡ä¸æ•°æ®åº“ä¸­å­˜å‚¨å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥å®ç°å¬å›ã€‚LangChainè¿˜æä¾›äº† `æ›´åŠ å¤æ‚çš„å¬å›ç­–ç•¥` ï¼Œè¿™äº›ç­–ç•¥è¢«é›†æˆåœ¨Retrieversï¼ˆæ£€ç´¢å™¨æˆ–å¬å›å™¨ï¼‰ç»„ä»¶ä¸­ã€‚

Retrieversï¼ˆæ£€ç´¢å™¨ï¼‰æ˜¯ä¸€ç§ç”¨äºä»å¤§é‡æ–‡æ¡£ä¸­æ£€ç´¢ä¸ç»™å®šæŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£æˆ–ä¿¡æ¯ç‰‡æ®µçš„å·¥å…·ã€‚æ£€ç´¢å™¨`ä¸éœ€è¦å­˜å‚¨æ–‡æ¡£` ï¼Œåªéœ€è¦ `è¿”å›ï¼ˆæˆ–æ£€ç´¢ï¼‰æ–‡æ¡£` å³å¯ã€‚

![alt text](/public/langchain/retrieval/7.png)

Retrievers çš„æ‰§è¡Œæ­¥éª¤ï¼š
æ­¥éª¤1ï¼šå°†è¾“å…¥æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚

æ­¥éª¤2ï¼šåœ¨å‘é‡å­˜å‚¨ä¸­æœç´¢ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„æ–‡æ¡£å‘é‡ï¼ˆé€šå¸¸ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–æ¬§å‡ é‡Œå¾—è·ç¦»ç­‰åº¦é‡æ–¹æ³•ï¼‰ã€‚

æ­¥éª¤3ï¼šè¿”å›ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£æˆ–æ–‡æœ¬ç‰‡æ®µï¼Œä»¥åŠå®ƒä»¬çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚

### 6.2 ä»£ç å®ç°
Retriever ä¸€èˆ¬å’Œ VectorStore é…å¥—å®ç°ï¼Œé€šè¿‡ `as_retriever()` æ–¹æ³•è·å–ã€‚

ä¸¾ä¾‹ï¼š

```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
import os
import dotenv

from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

dotenv.load_dotenv()

# 2.å®šä¹‰æ–‡æ¡£åŠ è½½å™¨
loader = TextLoader(file_path='./asset/load/09-ai1.txt',encoding="utf-8")
# 3.åŠ è½½æ–‡æ¡£
documents = loader.load()
# 4.å®šä¹‰æ–‡æœ¬åˆ‡å‰²å™¨
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
# 5.åˆ‡å‰²æ–‡æ¡£
docs = text_splitter.split_documents(documents)
# 6.å®šä¹‰åµŒå…¥æ¨¡å‹
os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")
embeddings = OpenAIEmbeddings(
  model="text-embedding-3-large"
)

# 7.å°†æ–‡æ¡£å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­
db = FAISS.from_documents(docs, embeddings)

# 8.ä»å‘é‡æ•°æ®åº“ä¸­å¾—åˆ°æ£€ç´¢å™¨
retriever = db.as_retriever()

# 9.ä½¿ç”¨æ£€ç´¢å™¨æ£€ç´¢
docs = retriever.invoke("æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ")

print(len(docs))
# 10.å¾—åˆ°ç»“æœ
for doc in docs:
  print(f"â­{doc}")
```

### 6.3 ä½¿ç”¨ç›¸å…³æ£€ç´¢ç­–ç•¥

å‰ç½®ä»£ç ï¼š
```python
# 1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
# 2.å®šä¹‰æ–‡æ¡£
document_1 = Document(
  page_content="ç»æµå¤è‹ï¼šç¾å›½ç»æµæ­£åœ¨ä»ç–«æƒ…ä¸­å¼ºåŠ²å¤è‹ï¼Œå¤±ä¸šç‡é™è‡³å†å²ä½ç‚¹ï¼
)

document_2 = Document(
  page_content="åŸºç¡€è®¾æ–½ï¼šæ”¿åºœå°†æŠ•èµ„1ä¸‡äº¿ç¾å…ƒç”¨äºä¿®å¤é“è·¯ã€æ¡¥æ¢å’Œå®½å¸¦ç½‘ç»œã€‚",
)

document_3 = Document(
page_content="æ°”å€™å˜åŒ–ï¼šæ‰¿è¯ºåˆ°2030å¹´å°†æ¸©å®¤æ°”ä½“æ’æ”¾é‡å‡å°‘50%ã€‚",

documents = [document_1,document_2,document_3]

# 3.åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings(
  model="text-embedding-3-large"
)

# 4.å°†æ–‡æ¡£å‘é‡åŒ–ï¼Œæ·»åŠ åˆ°å‘é‡æ•°æ®åº“ç´¢å¼•ä¸­ï¼Œå¾—åˆ°å‘é‡æ•°æ®åº“å¯¹è±¡
db = FAISS.from_documents(documents, embeddings)
```

**â‘  é»˜è®¤æ£€ç´¢å™¨ä½¿ç”¨ç›¸ä¼¼æ€§æœç´¢**
```python
# è·å–æ£€ç´¢å™¨
retriever = db.as_retriever(search_kwargs={"k": 4}) #è¿™é‡Œè®¾ç½®è¿”å›çš„æ–‡æ¡£æ•°

docs = retriever.invoke("ç»æµæ”¿ç­–")

for i, doc in enumerate(docs):
  print(f"\nç»“æœ {i+1}:\n{doc.page_content}\n")

```

**â‘¡ åˆ†æ•°é˜ˆå€¼æŸ¥è¯¢**

åªæœ‰ç›¸ä¼¼åº¦è¶…è¿‡è¿™ä¸ªå€¼æ‰ä¼šå¬å›

```python
retriever = db.as_retriever(
  search_type="similarity_score_threshold",
  search_kwargs={"score_threshold": 0.1}
)

docs = retriever.invoke("ç»æµæ”¿ç­–")

for doc in docs:
  print(f"ğŸ“Œ å†…å®¹: {doc.page_content}")

```
> ğŸ“Œ å†…å®¹: ç»æµå¤è‹ï¼šç¾å›½ç»æµæ­£åœ¨ä»ç–«æƒ…ä¸­å¼ºåŠ²å¤è‹ï¼Œå¤±ä¸šç‡é™è‡³å†å²ä½ç‚¹ã€‚ï¼

æ³¨æ„åªä¼šè¿”å›æ»¡è¶³é˜ˆå€¼åˆ†æ•°çš„æ–‡æ¡£ï¼Œä¸ä¼šè·å–æ–‡æ¡£çš„å¾—åˆ†ã€‚å¦‚æœæƒ³æŸ¥è¯¢æ–‡æ¡£çš„å¾—åˆ†æ˜¯å¦æ»¡è¶³é˜ˆå€¼ï¼Œå¯ä»¥ä½¿ç”¨å‘é‡æ•°æ®åº“çš„ similarity_search_with_relevance_scores æŸ¥çœ‹ï¼ˆåœ¨5.4.2 æƒ…å†µ5ä¸­è®²è¿‡ï¼‰ã€‚

```python
docs_with_scores = db.similarity_search_with_relevance_scores("ç»æµæ”¿ç­–")
for doc, score in docs_with_scores:
  print(f"\nç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}")
  print(f"ğŸ“Œ å†…å®¹: {doc.page_content}")

```

**â‘¢ MMRæœç´¢**

```python
retriever = db.as_retriever(
  search_type="mmr",
  # search_kwargs={"fetch_k":2}
)

docs = retriever.invoke("ç»æµæ”¿ç­–")
print(len(docs))
for doc in docs:
  print(f"ğŸ“Œ å†…å®¹: {doc.page_content}")
```

### 6.4 ç»“åˆLLM

ä¸¾ä¾‹1ï¼šé€šè¿‡FAISSæ„å»ºä¸€ä¸ªå¯æœç´¢çš„å‘é‡ç´¢å¼•æ•°æ®åº“ï¼Œå¹¶ç»“åˆRAGæŠ€æœ¯è®©LLMå»å›ç­”é—®é¢˜ã€‚

**æƒ…å†µ1ï¼šä¸ç”¨RAGç»™LLMçŒè¾“ä¸Šä¸‹æ–‡æ•°æ®**

```python
from langchain_openai import ChatOpenAI
import os
import dotenv
dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

# åˆ›å»ºå¤§æ¨¡å‹å®ä¾‹
llm = ChatOpenAI(model="gpt-4o-mini")
# è°ƒç”¨

response = llm.invoke("åŒ—äº¬æœ‰ä»€ä¹ˆè‘—åçš„å»ºç­‘ï¼Ÿ")
print(response.content)

```

**æƒ…å†µ2ï¼šä½¿ç”¨RAGç»™LLMçŒè¾“ä¸Šä¸‹æ–‡æ•°æ®**

> pip install faiss-cpu

```python
# 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åŒ…
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI,OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
import os
import dotenv

dotenv.load_dotenv()
# 2. åˆ›å»ºè‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
prompt_template = """è¯·ä½¿ç”¨ä»¥ä¸‹æä¾›çš„æ–‡æœ¬å†…å®¹æ¥å›ç­”é—®é¢˜ã€‚ä»…ä½¿ç”¨æä¾›çš„æ–‡æœ¬ä¿¡æ¯ï¼Œå¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”"æŠ±æ­‰ï¼Œæä¾›çš„æ–‡æœ¬ä¸­æ²¡æœ‰è¿™ä¸ªä¿¡æ¯"ã€‚
æ–‡æœ¬å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
"
"""
prompt = PromptTemplate.from_template(prompt_template)
# 3. åˆå§‹åŒ–æ¨¡å‹
os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")
llm = ChatOpenAI(
  model="gpt-4o-mini",
  temperature=0
)

embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")

# 4. åŠ è½½æ–‡æ¡£
loader = TextLoader("./asset/load/10-test_doc.txt", encoding='utf-8')
documents = loader.load()
# 5. åˆ†å‰²æ–‡æ¡£
text_splitter = CharacterTextSplitter(
  chunk_size=1000,
  chunk_overlap=100,
)
texts = text_splitter.split_documents(documents)
#print(f"æ–‡æ¡£ä¸ªæ•°:{len(texts)}")
# 6. åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = FAISS.from_documents(
  documents=texts,
  embedding=embedding_model
)
# 7.è·å–æ£€ç´¢å™¨
retriever = vectorstore.as_retriever()

docs = retriever.invoke("åŒ—äº¬æœ‰ä»€ä¹ˆè‘—åçš„å»ºç­‘ï¼Ÿ")
# 8. åˆ›å»ºRunnableé“¾
chain = prompt | llm
# 9. æé—®
result = chain.invoke(input={"question":"åŒ—äº¬æœ‰ä»€ä¹ˆè‘—åçš„å»ºç­‘ï¼Ÿ","context":docs})
print("\nå›ç­”:", result.content)
```